{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "c0ZxPq5iCeFe"
   },
   "source": [
    "# Priors and Forward Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 457
    },
    "id": "PtTFZDcx-Kx6",
    "outputId": "ff950c40-d714-4602-d5ee-20263dc15f02"
   },
   "outputs": [],
   "source": [
    "from importlib import reload \n",
    "import numpy as np\n",
    "import h5py\n",
    "from utils import *\n",
    "import time\n",
    "\n",
    "## Prior parameter ##\n",
    "nl_level = 10\n",
    "\n",
    "PRIOR = np.ones(4, dtype=[('MU', float, (4)), ('C', float, (4,4))])\n",
    "\n",
    "mu = np.ones([4,4])\n",
    "mu[0] = [8.5948, 8.0356, 0.9613, 9.5561]\n",
    "mu[1] = [8.4563, 7.8829, 0.9186, 9.3749]\n",
    "mu[2] = [8.6676, 8.0483, 1.0716, 9.7392]\n",
    "mu[3] = [8.4136, 7.8345, 0.7348, 9.1484]\n",
    "#print(mu)\n",
    "\n",
    "c = np.ones([4,4,4])\n",
    "c[0,:] = [[0.0037, 0.0027, 0.0008, 0.0000],\n",
    "            [0.0027, 0.0030, 0.0008, 0.0000],\n",
    "            [0.0008, 0.0008, 0.0003, 0.0000],\n",
    "            [0.0000, 0.0000, 0.0000, 0.0111]]\n",
    "\n",
    "c[1,:] = [[0.0032, 0.0028, 0.0011, 0.0000],\n",
    "            [0.0028, 0.0033, 0.0010, 0.0000],\n",
    "            [0.0011, 0.0010, 0.0006, 0.0000],\n",
    "            [0.0000, 0.0000, 0.0000, 0.0096]]\n",
    "\n",
    "c[2,:] = [[0.0023, 0.0016, 0.0008, 0.0000],\n",
    "            [0.0016, 0.0017, 0.0007, 0.0000],\n",
    "            [0.0008, 0.0007, 0.0006, 0.0000],\n",
    "            [0.0000, 0.0000, 0.0000, 0.0070]]\n",
    "\n",
    "c[3,:] = [[0.0008, 0.0005, 0.0003, 0.0000],\n",
    "            [0.0005, 0.0004, 0.0002, 0.0000],\n",
    "            [0.0003, 0.0002, 0.0003, 0.0000],\n",
    "            [0.0000, 0.0000, 0.0000, 0.0023]]\n",
    "#print(c)\n",
    "\n",
    "wavelet_new = [-1039.55055580727,\n",
    "            -4026.57620068033,\n",
    "            -8194.18572794536,\n",
    "            -11356.0387947636,\n",
    "            -11906.6435543803,\n",
    "            -10191.4389229172,\n",
    "            -8639.29324618705,\n",
    "            -10474.3682826362,\n",
    "            -17093.5087710456,\n",
    "            -25733.4288332681,\n",
    "            -29906.2345220425,\n",
    "            -23088.0727988944,\n",
    "            -3091.80943987738,\n",
    "            25306.1208726887,\n",
    "            51180.4184438887,\n",
    "            63288.5822013372,\n",
    "            53619.9099732780,\n",
    "            32940.2018555759,\n",
    "            11743.5370194762,\n",
    "            -3175.56289063239,\n",
    "            -10845.0261972202,\n",
    "            -13246.6910319655,\n",
    "            -12569.5536619989,\n",
    "            -10329.2514738468,\n",
    "            -7512.57465309683,\n",
    "            -5018.31852539635,\n",
    "            -3551.82629919458,\n",
    "            -3000.47980521376,\n",
    "            -2436.41725067266,\n",
    "            -1305.36051838710,\n",
    "            -346.558408174331]\n",
    "\n",
    "PRIOR['MU'] = mu\n",
    "PRIOR['C'] = c\n",
    "\n",
    "## Geracao do ensemble ##\n",
    "n = 48\n",
    "I = n\n",
    "J = 1\n",
    "signal2noise = 5\n",
    "v_fact = 0.1\n",
    "\n",
    "wavelet = np.array(wavelet_new)\n",
    "\n",
    "delta = np.zeros([31,1])\n",
    "delta[np.around(delta.shape[0]/2).astype(int)-1,0] = 1\n",
    "\n",
    "wavelet = lowPassFilter2(delta,4,40,60) - lowPassFilter2(delta,4,40,6)\n",
    "\n",
    "G = acoustic_foward_matrix(wavelet,I)\n",
    "\n",
    "#P = np.matrix('0.7 0.3 0 0; 0.3 0.7 0 0; 0.33 0.33 0.34 0; 0.1 0.1 0.1 0.7')\n",
    "P = np.matrix('0.7 0.3 0 0; 0.3 0.7 0 0; 0.33 0.33 0.34 0; 0.1 0.1 0.1 0.7')\n",
    "P = np.array(P)\n",
    "\n",
    "facies = simulate_markov_chain(P,n,3,1)\n",
    "print(facies.shape)\n",
    "mu, log_imp, seismic = facies_forward_model(facies, PRIOR, G, v_fact)\n",
    "\n",
    "noise = np.random.randn(I-1,1)\n",
    "noise = noise/np.std(noise)\n",
    "std_noise = np.std(seismic)/np.sqrt(signal2noise)\n",
    "noise = noise*std_noise\n",
    "seismic = seismic + noise.ravel()\n",
    "\n",
    "fig, axs = plt.subplots(4)\n",
    "fig.set_dpi(120)\n",
    "axs[0].imshow(facies.transpose(), aspect='auto')\n",
    "axs[1].plot(seismic)\n",
    "axs[2].plot(log_imp)\n",
    "axs[3].plot(wavelet)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DarvZDngIjHG"
   },
   "source": [
    "# Data and GAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 256
    },
    "id": "XbKrz8C9XjnF",
    "outputId": "545d0fce-82fc-40e4-9573-e030803fc8fc"
   },
   "outputs": [],
   "source": [
    "from scipy.ndimage.filters import gaussian_filter\n",
    "\n",
    "Pver = np.array(np.matrix('0.7 0.3 0 0; 0.3 0.7 0 0; 0.33 0.33 0.34 0; 0.066 0.066 0.066 0.802'))\n",
    "Phor = np.array(np.matrix('0.4 0.4 0.1 0.1; 0.4 0.4 0.1 0.1; 0.1 0.1 0.6 0.2; 0.1 0.1 0.2 0.6'))\n",
    "\n",
    "#Simulation grid size\n",
    "\n",
    "I = 48\n",
    "J = 48\n",
    "initial_facies = 3\n",
    "\n",
    "prior_map = np.ones([I, J, 4])\n",
    "\n",
    "simulation = simulate_markov_2Dchain(Phor, Pver, prior_map, initial_facies)\n",
    "ss = gaussian_filter(simulation, sigma=[2, 1])\n",
    "st = simulation == 3\n",
    "ss[st] = 3\n",
    "ss = np.round(gaussian_filter(ss, sigma=[1.2, 1]))\n",
    "\n",
    "def facies_forward_model_2D(facies, PRIOR, G, v_fact):\n",
    "  seismics = []\n",
    "  impedances = []\n",
    "  for j in range(0,J):\n",
    "    mu, log_imp, seismic = facies_forward_model(facies[:,j], PRIOR, G, v_fact)\n",
    "    seismics.append(seismic)\n",
    "    impedances.append(log_imp)\n",
    "\n",
    "  seismics = np.array(seismics).transpose()\n",
    "  impedances = np.array(impedances).transpose()\n",
    "  return seismics, impedances\n",
    "\n",
    "seis, immp = facies_forward_model_2D(ss, PRIOR, G, v_fact)\n",
    "\n",
    "fig, axes = plt.subplots(nrows=1, ncols=3, figsize=(10, 6))\n",
    "axes[0].imshow(ss)\n",
    "axes[1].imshow(seis, cmap='gray')\n",
    "axes[2].imshow(immp)\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 537
    },
    "id": "dUvQ4UBmJcg_",
    "outputId": "d3123d6a-f29e-4eb9-d244-c7d9f6f1e3cb"
   },
   "outputs": [],
   "source": [
    "def create_train_data(data_size = 512, i = 48, j = 48):\n",
    "  initial_facies = 3\n",
    "  prior_map = np.ones([I, J, 4])\n",
    "  \n",
    "  data = np.zeros([data_size, i, j])\n",
    "  for i in range(0, data_size):\n",
    "    simulation = simulate_markov_2Dchain(Phor, Pver, prior_map, initial_facies)\n",
    "    ss = gaussian_filter(simulation, sigma=[2, 1])\n",
    "    st = simulation == 3\n",
    "    ss[st] = 3\n",
    "    ss = np.round(gaussian_filter(ss, sigma=[1.2, 1]))\n",
    "    data[i] = ss\n",
    "  return data\n",
    "\n",
    "start_time = time.time()\n",
    "X_bk = create_train_data()\n",
    "fig = plt.figure(dpi=150)\n",
    "fig.colorbar(plt.imshow(X_bk[0]))\n",
    "plt.axis('off')\n",
    "plt.show()\n",
    "#print(X_bk)\n",
    "#X_bk = np.expand_dims(X_bk.transpose(), axis=2)\n",
    "print('Data generation time (s): ', (time.time() - start_time))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fmm6ReQOU8Ho"
   },
   "source": [
    "# Save data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "TDPb2DF5UXWc",
    "outputId": "af4327cf-bdf2-4e14-b4fe-38ad35a370cf"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import h5py\n",
    "\n",
    "hf = h5py.File('data_set_2D.h5', 'w')\n",
    "hf.create_dataset('X', data=X_bk)\n",
    "hf.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "s7hl0B2vVRFZ",
    "outputId": "a9f465ab-4dbb-4380-95a5-ff32c6aaf5c6"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import h5py\n",
    "hf = h5py.File('data_set_2D.h5', 'r')\n",
    "X_bk = np.array(hf['X'])\n",
    "hf.close()\n",
    "print(X_bk.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Process data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.copy(X_bk).astype(dtype='float32')\n",
    "X /= X.max()\n",
    "#X = (X - 0.5) / 0.5 # normalize (-1,1)\n",
    "X = np.expand_dims(X, axis=3)\n",
    "#X = np.transpose(X, axes=(0,2,1,3))\n",
    "#X = tf.keras.utils.to_categorical(X)\n",
    "print(X.shape)\n",
    "print(X_bk.min(),X_bk.max())\n",
    "print(X.min(),X.max())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zjXc4OLSNpBM"
   },
   "source": [
    "# Convolutional Variational Autoencoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 609
    },
    "id": "pZQULnwYNmpa",
    "outputId": "6855ca5d-b52b-4c07-a60b-02fcc1397967"
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function, division\n",
    "\n",
    "from IPython.display import clear_output\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "\n",
    "from tensorflow.keras.datasets import mnist\n",
    "from tensorflow.keras.layers import Input, Dense, Reshape, Flatten, Dropout\n",
    "from tensorflow.keras.layers import BatchNormalization, Activation, ZeroPadding2D, LeakyReLU\n",
    "from tensorflow.keras.layers import UpSampling2D, Conv2D, Conv2DTranspose\n",
    "from tensorflow.keras.models import Sequential, Model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "import tensorflow_probability as tfp\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import sys\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "class CVAE_Dense(tf.keras.Model):\n",
    "    def __init__(self):\n",
    "        super(CVAE_Dense, self).__init__()\n",
    "        # Input shape\n",
    "        self.img_rows = 48\n",
    "        self.img_cols = 48\n",
    "        self.channels = 1\n",
    "        self.img_shape = (self.img_rows, self.img_cols, self.channels)\n",
    "        self.latent_dim = 144\n",
    "\n",
    "        self.optimizer = Adam(1e-4)\n",
    "\n",
    "        self.encoder = self.build_encoder()\n",
    "        self.decoder = self.build_decoder()\n",
    "\n",
    "    def encode(self, x):\n",
    "      mean, logvar = tf.split(self.encoder(x), num_or_size_splits=2, axis=1)\n",
    "      print(\"encoder: \", self.encoder(x).shape)\n",
    "      print(\"mean: \", mean.shape)\n",
    "      return mean, logvar\n",
    "\n",
    "    def decode(self, z, apply_sigmoid=False):\n",
    "      logits = self.decoder(z)\n",
    "      if apply_sigmoid:\n",
    "        probs = tf.sigmoid(logits)\n",
    "        return probs\n",
    "      return logits\n",
    "\n",
    "    def reparameterize(self, mean, logvar):\n",
    "      eps = tf.random.normal(shape=mean.shape)\n",
    "      return eps * tf.exp(logvar * .5) + mean\n",
    "\n",
    "    @tf.function\n",
    "    def sample(self, eps=None):\n",
    "      if eps is None:\n",
    "        eps = tf.random.normal(shape=(25, self.latent_dim))\n",
    "      return self.decode(eps, apply_sigmoid=True)\n",
    "\n",
    "    def log_normal_pdf(self, sample, mean, logvar, raxis=1):\n",
    "      log2pi = tf.math.log(2. * np.pi)\n",
    "      return tf.reduce_sum(-.5 * ((sample - mean) ** 2. * tf.exp(-logvar) + logvar + log2pi), axis=raxis)\n",
    "\n",
    "    def compute_loss(self, x):\n",
    "      mean, logvar = self.encode(x)\n",
    "      z = self.reparameterize(mean, logvar)\n",
    "      x_logit = self.decode(z)\n",
    "      cross_ent = tf.nn.sigmoid_cross_entropy_with_logits(logits=x_logit, labels=x)\n",
    "      logpx_z = -tf.reduce_sum(cross_ent, axis=[1, 2, 3])\n",
    "      logpz = self.log_normal_pdf(z, 0., 0.)\n",
    "      logqz_x = self.log_normal_pdf(z, mean, logvar)\n",
    "      return -tf.reduce_mean(logpx_z + logpz - logqz_x)\n",
    "\n",
    "    def build_encoder(self):\n",
    "\n",
    "        model = Sequential()\n",
    "        model.add(Input(shape=self.img_shape))\n",
    "        model.add(Conv2D(32, kernel_size=3, strides=2, activation=\"relu\", padding=\"same\"))\n",
    "        model.add(Conv2D(64, kernel_size=3, strides=2, activation=\"relu\", padding=\"same\"))\n",
    "        model.add(Conv2D(128, kernel_size=3, strides=2, activation=\"relu\", padding=\"same\"))\n",
    "        model.add(Flatten())\n",
    "        model.add(Dense(self.latent_dim + self.latent_dim))\n",
    "        model.summary()\n",
    "\n",
    "        return model\n",
    "\n",
    "    def build_decoder(self):\n",
    "\n",
    "        model = Sequential()\n",
    "        model.add(Input(shape=(self.latent_dim,)))\n",
    "        model.add(Dense(6 * 6 * 32, activation=\"relu\"))\n",
    "        model.add(Reshape((6, 6, 32)))\n",
    "        model.add(Conv2DTranspose(128, kernel_size=3, strides=2, padding=\"same\", activation=\"relu\"))\n",
    "        model.add(Conv2DTranspose(64, kernel_size=3, strides=2, padding=\"same\", activation=\"relu\"))\n",
    "        model.add(Conv2DTranspose(32, kernel_size=3, strides=2, padding=\"same\", activation=\"relu\"))\n",
    "        #No activation\n",
    "        model.add(Conv2DTranspose(1, kernel_size=3, strides=1, padding=\"same\"))\n",
    "\n",
    "        model.summary()\n",
    "\n",
    "        return model\n",
    "\n",
    "    @tf.function\n",
    "    def train_step(self, x):\n",
    "      print(self.trainable_variables)\n",
    "      with tf.GradientTape() as tape:\n",
    "        loss = self.compute_loss(x)\n",
    "      gradients = tape.gradient(loss, self.trainable_variables)\n",
    "      self.optimizer.apply_gradients(zip(gradients, self.trainable_variables))\n",
    "\n",
    "    def train(self, epochs, batch_size=128, save_interval=50):\n",
    "\n",
    "        X_train = X\n",
    "        \n",
    "        noise = tf.random.normal(shape=[batch_size, self.latent_dim])\n",
    "\n",
    "        for epoch in range(epochs):\n",
    "\n",
    "            idx = np.random.randint(0, X_train.shape[0], batch_size)\n",
    "            imgs = X_train[idx]\n",
    "\n",
    "            start_time = time.time()\n",
    "            #for img in imgs:\n",
    "            self.train_step(imgs)\n",
    "            end_time = time.time()\n",
    "\n",
    "            if epoch % save_interval == 0:\n",
    "              loss = tf.keras.metrics.Mean()\n",
    "              #for img in imgs:\n",
    "              loss(self.compute_loss(imgs))\n",
    "              elbo = -loss.result()\n",
    "              clear_output(wait=True)\n",
    "              print('Epoch: {}, Test set ELBO: {}, time elapse for current epoch: {}'\n",
    "                    .format(epoch, elbo, end_time - start_time))\n",
    "              self.save_imgs(epoch, X_train[np.random.randint(0, X_train.shape[0], 16)])\n",
    "\n",
    "    def save_imgs(self, epoch, test_sample):\n",
    "      mean, logvar = self.encode(test_sample)\n",
    "      z = self.reparameterize(mean, logvar)\n",
    "      z = tf.random.normal(shape=[16, self.latent_dim])\n",
    "      print(z.shape)\n",
    "      predictions = self.sample(z)\n",
    "      fig = plt.figure(figsize=(10, 10))\n",
    "\n",
    "      for i in range(predictions.shape[0]):\n",
    "        plt.subplot(4, 4, i + 1)\n",
    "        plt.imshow(np.around(predictions[i, :, :, 0]*3))\n",
    "        plt.axis('off')\n",
    "\n",
    "      # tight_layout minimizes the overlap between 2 sub-plots\n",
    "      #plt.savefig('image_at_epoch_{:04d}.png'.format(epoch))\n",
    "      plt.show()\n",
    "\n",
    "    def plot_latent_images(self, n=20, digit_size=48):\n",
    "      \"\"\"Plots n x n digit images decoded from the latent space.\"\"\"\n",
    "\n",
    "      norm = tfp.distributions.Normal(0, 1)\n",
    "      grid_x = norm.quantile(np.linspace(0.05, 0.95, n))\n",
    "      grid_y = norm.quantile(np.linspace(0.05, 0.95, n))\n",
    "      image_width = digit_size*n\n",
    "      image_height = image_width\n",
    "      image = np.zeros((image_height, image_width))\n",
    "\n",
    "      for i, yi in enumerate(grid_x):\n",
    "        for j, xi in enumerate(grid_y):\n",
    "          z = np.array([[xi, yi]])\n",
    "          x_decoded = self.sample(z)\n",
    "          digit = tf.reshape(x_decoded[0], (digit_size, digit_size))\n",
    "          image[i * digit_size: (i + 1) * digit_size,\n",
    "                j * digit_size: (j + 1) * digit_size] = digit.numpy()\n",
    "\n",
    "      plt.figure(figsize=(10, 10))\n",
    "      plt.imshow(np.around(image*3))\n",
    "      plt.axis('Off')\n",
    "      plt.show()\n",
    "\n",
    "\n",
    "\n",
    "cvae_dense = CVAE_Dense()\n",
    "cvae_dense.train(epochs=1001, batch_size=128, save_interval=500)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FK9jqoUTjUoS"
   },
   "source": [
    "# CVAE Fully Conv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "5W7lqBILe-Fk",
    "outputId": "b9122fd0-d75e-4b37-9122-4cb922036568"
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function, division\n",
    "\n",
    "from IPython.display import clear_output\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "\n",
    "from tensorflow.keras.datasets import mnist\n",
    "from tensorflow.keras.layers import Input, Dense, Reshape, Flatten, Dropout\n",
    "from tensorflow.keras.layers import BatchNormalization, Activation, ZeroPadding2D, LeakyReLU\n",
    "from tensorflow.keras.layers import UpSampling2D, Conv2D, Conv2DTranspose\n",
    "from tensorflow.keras.models import Sequential, Model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "import tensorflow_probability as tfp\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import sys\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "class CVAE(tf.keras.Model):\n",
    "    def __init__(self):\n",
    "        super(CVAE, self).__init__()\n",
    "        # Input shape\n",
    "        self.img_rows = 48\n",
    "        self.img_cols = 48\n",
    "        self.channels = 1\n",
    "        self.img_shape = (self.img_rows, self.img_cols, self.channels)\n",
    "        self.latent_dim = 12\n",
    "        self.latent_dim_shape = (self.latent_dim, self.latent_dim, self.channels)\n",
    "\n",
    "        self.optimizer = Adam(1e-4)\n",
    "\n",
    "        self.encoder = self.build_encoder()\n",
    "        self.decoder = self.build_decoder()\n",
    "\n",
    "    def encode(self, x):\n",
    "      mean, logvar = tf.split(self.encoder(x), num_or_size_splits=2, axis=-1)\n",
    "      return mean, logvar\n",
    "\n",
    "    def decode(self, z, apply_sigmoid=False):\n",
    "      logits = self.decoder(z)\n",
    "      if apply_sigmoid:\n",
    "        probs = tf.sigmoid(logits)\n",
    "        return probs\n",
    "      return logits\n",
    "\n",
    "    def reparameterize(self, mean, logvar):\n",
    "      eps = tf.random.normal(shape=mean.shape)\n",
    "      return eps * tf.exp(logvar * .5) + mean\n",
    "\n",
    "    @tf.function\n",
    "    def sample(self, eps=None):\n",
    "      if eps is None:\n",
    "        eps = tf.random.normal(shape=(25, self.latent_dim, self.latent_dim, self.channels))\n",
    "      return self.decode(eps, apply_sigmoid=True)\n",
    "\n",
    "    def log_normal_pdf(self, sample, mean, logvar, raxis=1):\n",
    "      log2pi = tf.math.log(2. * np.pi)\n",
    "      return tf.reduce_sum(-.5 * ((sample - mean) ** 2. * tf.exp(-logvar) + logvar + log2pi), axis=raxis)\n",
    "\n",
    "    def compute_loss(self, x):\n",
    "      mean, logvar = self.encode(x)\n",
    "      print(mean.shape)\n",
    "      z = self.reparameterize(mean, logvar)\n",
    "      x_logit = self.decode(z)\n",
    "      cross_ent = tf.nn.sigmoid_cross_entropy_with_logits(logits=x_logit, labels=x)\n",
    "      logpx_z = -tf.reduce_sum(cross_ent, axis=[1, 2, 3])\n",
    "      logpz = self.log_normal_pdf(z, 0., 0.)\n",
    "      logqz_x = self.log_normal_pdf(z, mean, logvar)\n",
    "      return -tf.reduce_mean(logpx_z + logpz - logqz_x)\n",
    "\n",
    "    def build_encoder(self):\n",
    "\n",
    "        model = Sequential()\n",
    "        model.add(Input(shape=self.img_shape))\n",
    "        model.add(Conv2D(32, kernel_size=3, strides=2, activation=\"relu\", padding=\"same\")) # size 24\n",
    "        model.add(Conv2D(64, kernel_size=3, strides=1, activation=\"relu\", padding=\"same\")) # size 12\n",
    "        model.add(Conv2D(128, kernel_size=3, strides=2, activation=\"relu\", padding=\"same\")) # size 6\n",
    "        model.add(Conv2D(128, kernel_size=3, strides=1, activation=\"relu\", padding=\"same\")) # size 6\n",
    "        #model.add(Flatten())\n",
    "        #model.add(Dense(self.latent_dim + self.latent_dim))\n",
    "        model.add(Conv2D(2, kernel_size=1, strides=1, padding=\"same\"))\n",
    "        model.summary()\n",
    "\n",
    "        return model\n",
    "\n",
    "    def build_decoder(self):\n",
    "\n",
    "        model = Sequential()\n",
    "        model.add(Input(shape=(self.latent_dim_shape)))\n",
    "        model.add(Conv2DTranspose(2, kernel_size=1, strides=1, padding=\"same\", activation=\"relu\"))\n",
    "        #model.add(Dense(6 * 6 * 32, activation=\"relu\"))\n",
    "        #model.add(Reshape((6, 6, 32)))\n",
    "        model.add(Conv2DTranspose(128, kernel_size=3, strides=1, padding=\"same\", activation=\"relu\")) # size 6\n",
    "        model.add(Conv2DTranspose(128, kernel_size=3, strides=2, padding=\"same\", activation=\"relu\")) # size 12\n",
    "        model.add(Conv2DTranspose(64, kernel_size=3, strides=1, padding=\"same\", activation=\"relu\")) # size 24\n",
    "        model.add(Conv2DTranspose(32, kernel_size=3, strides=2, padding=\"same\", activation=\"relu\")) #size 48\n",
    "        #No activation\n",
    "        model.add(Conv2DTranspose(1, kernel_size=3, strides=1, padding=\"same\"))\n",
    "\n",
    "        model.summary()\n",
    "\n",
    "        return model\n",
    "\n",
    "    @tf.function\n",
    "    def train_step(self, x):\n",
    "      with tf.GradientTape() as tape:\n",
    "        loss = self.compute_loss(x)\n",
    "      gradients = tape.gradient(loss, self.trainable_variables)\n",
    "      self.optimizer.apply_gradients(zip(gradients, self.trainable_variables))\n",
    "\n",
    "    def train(self, epochs, batch_size=128, save_interval=50):\n",
    "\n",
    "        X_train = X\n",
    "        \n",
    "        noise = tf.random.normal(shape=[batch_size, self.latent_dim, self.latent_dim])\n",
    "\n",
    "        for epoch in range(epochs):\n",
    "\n",
    "            idx = np.random.randint(0, X_train.shape[0], batch_size)\n",
    "            imgs = X_train[idx]\n",
    "\n",
    "            start_time = time.time()\n",
    "            #for img in imgs:\n",
    "            self.train_step(imgs)\n",
    "            end_time = time.time()\n",
    "\n",
    "            if epoch % save_interval == 0:\n",
    "              loss = tf.keras.metrics.Mean()\n",
    "              #for img in imgs:\n",
    "              loss(self.compute_loss(imgs))\n",
    "              elbo = -loss.result()\n",
    "              clear_output(wait=True)\n",
    "              print('Epoch: {}, Test set ELBO: {}, time elapse for current epoch: {}'\n",
    "                    .format(epoch, elbo, end_time - start_time))\n",
    "              self.save_imgs(epoch, X_train[np.random.randint(0, X_train.shape[0], 16)])\n",
    "\n",
    "    def save_imgs(self, epoch, test_sample):\n",
    "      mean, logvar = self.encode(test_sample)\n",
    "      z = self.reparameterize(mean, logvar)\n",
    "      z = tf.random.normal(shape=[16,self.latent_dim, self.latent_dim, 1])\n",
    "      print(z.shape, mean.shape)\n",
    "      fig, axs = plt.subplots(1,2)\n",
    "      axs[0].hist(mean.numpy().flatten())\n",
    "      axs[0].set_title('mean true')\n",
    "      axs[1].hist(z.numpy().flatten())\n",
    "      axs[1].set_title('z from normal')\n",
    "      plt.show()\n",
    "      predictions = self.sample(eps=z)\n",
    "      fig = plt.figure(figsize=(10, 10))\n",
    "\n",
    "      for i in range(predictions.shape[0]):\n",
    "        plt.subplot(4, 4, i + 1)\n",
    "        plt.imshow(np.around(predictions[i, :, :, 0]*3))\n",
    "        plt.axis('off')\n",
    "\n",
    "      # tight_layout minimizes the overlap between 2 sub-plots\n",
    "      #plt.savefig('image_at_epoch_{:04d}.png'.format(epoch))\n",
    "      plt.show()\n",
    "\n",
    "    def plot_latent_images(self, n=20, digit_size=48):\n",
    "      \"\"\"Plots n x n digit images decoded from the latent space.\"\"\"\n",
    "\n",
    "      norm = tfp.distributions.Normal(0, 1)\n",
    "      grid_x = norm.quantile(np.linspace(0.05, 0.95, n))\n",
    "      grid_y = norm.quantile(np.linspace(0.05, 0.95, n))\n",
    "      image_width = digit_size*n\n",
    "      image_height = image_width\n",
    "      image = np.zeros((image_height, image_width))\n",
    "\n",
    "      for i, yi in enumerate(grid_x):\n",
    "        for j, xi in enumerate(grid_y):\n",
    "          z = np.array([[xi, yi]])\n",
    "          x_decoded = self.sample(z)\n",
    "          digit = tf.reshape(x_decoded[0], (digit_size, digit_size))\n",
    "          image[i * digit_size: (i + 1) * digit_size,\n",
    "                j * digit_size: (j + 1) * digit_size] = digit.numpy()\n",
    "\n",
    "      plt.figure(figsize=(10, 10))\n",
    "      plt.imshow(np.around(image*3))\n",
    "      plt.axis('Off')\n",
    "      plt.show()\n",
    "\n",
    "\n",
    "\n",
    "cvae = CVAE()\n",
    "cvae.train(epochs=1001, batch_size=128, save_interval=500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cvae.encoder.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cvae.decoder.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "p9hsqnjm4yNd"
   },
   "source": [
    "# Tests CVAE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "norm = tfp.distributions.Normal(0, 1)\n",
    "norm.cdf(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "dJn2nLno42k6",
    "outputId": "9e418787-dc75-4a26-a8b4-5cadfec3bc3f"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow_probability as tfp\n",
    "\n",
    "qtd = 50\n",
    "inputn = tf.random.normal(shape=[1, cvae.latent_dim, cvae.latent_dim, 1])\n",
    "\n",
    "plt.hist(inputn.numpy().flatten())\n",
    "plt.show()\n",
    "\n",
    "outn_dense = cvae_dense.sample(eps=tf.reshape(inputn,[1,-1])).numpy()\n",
    "im_dense = np.round(outn_dense[0].reshape(48,48)*3)\n",
    "\n",
    "outn = cvae.sample(eps=inputn).numpy()\n",
    "im = np.round(outn[0].reshape(48,48)*3)\n",
    "\n",
    "fig, axs = plt.subplots(1,2, figsize=(10, 6))\n",
    "axs[0].imshow(im_dense)\n",
    "axs[0].set_title('CVAE Dense')\n",
    "axs[1].imshow(im)\n",
    "axs[1].set_title('CVAE Conv')\n",
    "plt.show()\n",
    "\n",
    "qtd = 10\n",
    "\n",
    "im_test = ss.reshape(1,ss.shape[0], -1, 1)/3\n",
    "\n",
    "mean, logvar = cvae.encode(im_test)\n",
    "out = cvae.sample(cvae.reparameterize(mean, logvar))\n",
    "\n",
    "mean_dense, logvar_dense = cvae_dense.encode(im_test)\n",
    "out_dense = cvae_dense.sample(cvae_dense.reparameterize(mean_dense, logvar_dense))\n",
    "\n",
    "fig, axs = plt.subplots(1,2, figsize=(10, 6))\n",
    "axs[0].hist(mean_dense.numpy().flatten())\n",
    "axs[0].set_title('CVAE Dense')\n",
    "axs[1].hist(mean.numpy().flatten())\n",
    "axs[1].set_title('CVAE Conv')\n",
    "plt.show()\n",
    "\n",
    "#fig = plt.figure(dpi=100)\n",
    "#fig.colorbar(plt.imshow(im_test.reshape(I,J)))\n",
    "#plt.title('True')\n",
    "#plt.show()\n",
    "\n",
    "fig, axs = plt.subplots(1,5, figsize=(15, 8))\n",
    "axs[0].imshow(im_test.reshape(I,J))\n",
    "axs[0].set_title('True')\n",
    "axs[1].imshow(tf.reshape(out_dense, [n,n]))\n",
    "axs[1].set_title('CVAE Dense')\n",
    "axs[2].imshow(tf.reshape(out,[n,n]))\n",
    "axs[2].set_title('CVAE Conv')\n",
    "axs[3].imshow(tf.reshape(mean_dense,[cvae.latent_dim,cvae.latent_dim]))\n",
    "axs[3].set_title('CVAE Dense latent')\n",
    "axs[4].imshow(tf.reshape(mean,[cvae.latent_dim,cvae.latent_dim]))\n",
    "axs[4].set_title('CVAE Conv latent')\n",
    "plt.show()\n",
    "\n",
    "#plt.figure(dpi=100)\n",
    "#plt.hist([np.round(out*3).reshape(-1),  \n",
    "#          X_bk[:qtd].reshape(-1)], label=['GAN', 'True'])\n",
    "#plt.legend(loc='upper right')\n",
    "#plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CVAE Dense vs CVAE Conv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 48"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "im_test = ss.reshape(1,ss.shape[0], -1, 1)/3\n",
    "\n",
    "mean_dense, logvar_dense = cvae_dense.encode(im_test)\n",
    "z_dense = tf.Variable(cvae_dense.reparameterize(mean_dense, logvar_dense))\n",
    "\n",
    "mean, logvar = cvae.encode(im_test)\n",
    "z = tf.Variable(cvae.reparameterize(mean, logvar))\n",
    "\n",
    "shape_dense = [0,0]\n",
    "shape = [0,0,0,0]\n",
    "\n",
    "scalar = 0.5\n",
    "\n",
    "for i in range(0,21):\n",
    "    clear_output(wait=True)\n",
    "\n",
    "    z_dense[shape_dense].assign(z_dense[shape_dense] + scalar)\n",
    "    z[shape].assign(z[shape] + scalar)\n",
    "\n",
    "    out_dense = cvae_dense.sample(z_dense)\n",
    "    out = cvae.sample(z)\n",
    "\n",
    "    fig, axs = plt.subplots(1,2, figsize=(10, 6))\n",
    "    axs[0].imshow(tf.reshape(out_dense, [n,n]))\n",
    "    axs[1].imshow(tf.reshape(out,[n,n]))\n",
    "    fig.suptitle(('Itr ' + str(i)), fontsize=16)\n",
    "    plt.show()\n",
    "    time.sleep(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kh_n9mMi6Xes"
   },
   "source": [
    "# Save models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Dense\n",
    "cvae_dense.encoder.save('cvae_dense_encoder')\n",
    "cvae_dense.decoder.save('cvae_dense_decoder')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "BPozw97CS2ZC",
    "outputId": "04c6974c-4378-40e9-ac72-04782d0308c9"
   },
   "outputs": [],
   "source": [
    "#Fully Conv\n",
    "cvae.encoder.save('cvae_encoder')\n",
    "cvae.decoder.save('cvae_decoder')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cvae.encoder = tf.keras.models.load_model('cvae_encoder')\n",
    "cvae.decoder = tf.keras.models.load_model('cvae_decoder')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cvae_dense.encoder = tf.keras.models.load_model('cvae_dense_encoder')\n",
    "cvae_dense.decoder = tf.keras.models.load_model('cvae_dense_decoder')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GoXnAy1a8HBi"
   },
   "source": [
    "# Ensemble Smoother CVAE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plt_ensamble(modelo_samples):\n",
    "    w, h = int(math.sqrt(n_samples)), int(math.sqrt(n_samples))    \n",
    "    fig, axs = plt.subplots(w,h, figsize=(15, 15))\n",
    "    gan_facies = tf.experimental.numpy.around(cvae.sample(eps=modelo_samples)*3)\n",
    "    idx = 0\n",
    "    for i in range(0,w):\n",
    "        for j in range(0,h):\n",
    "            axs[i,j].imshow(tf.reshape(gan_facies[idx], [n,n]))\n",
    "            axs[i,j].axis('off')\n",
    "            #axs[i,j].set_xticklabels([])\n",
    "            #axs[i,j].set_yticklabels([])\n",
    "            #axs[i,j].set_aspect('equal')\n",
    "            idx = idx + 1\n",
    "    #fig.tight_layout()\n",
    "    plt.show()\n",
    "def plt_ensamble_dense(modelo_samples):\n",
    "    w, h = int(math.sqrt(n_samples)), int(math.sqrt(n_samples))    \n",
    "    fig, axs = plt.subplots(w,h, figsize=(15, 15))\n",
    "    gan_facies = tf.experimental.numpy.around(cvae_dense.sample(eps=modelo_samples[idx])*3)\n",
    "    idx = 0\n",
    "    for i in range(0,w):\n",
    "        for j in range(0,h):\n",
    "            axs[i,j].imshow(tf.reshape(gan_facies[idx], [n,n]))\n",
    "            axs[i,j].axis('off')\n",
    "            #axs[i,j].set_xticklabels([])\n",
    "            #axs[i,j].set_yticklabels([])\n",
    "            #axs[i,j].set_aspect('equal')\n",
    "            idx = idx + 1\n",
    "    #fig.tight_layout()\n",
    "    plt.show()\n",
    "#plt_ensamble(modelo_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "samples_init = tf.Variable(np.random.normal(size=(1, cvae.latent_dim, cvae.latent_dim, 1)), dtype=tf.float32)\n",
    "\n",
    "plt.hist(samples_init.numpy().flatten())\n",
    "plt.show()\n",
    "\n",
    "plt.imshow(tf.reshape(cvae.sample(eps=samples_init), [n,n]))\n",
    "plt.show()\n",
    "plt.close()\n",
    "\n",
    "samples_init = tf.Variable(np.random.normal(size=(1,100)), dtype=tf.float32)\n",
    "\n",
    "plt.hist(samples_init.numpy().flatten())\n",
    "plt.show()\n",
    "\n",
    "plt.imshow(tf.reshape(cvae_dense.sample(eps=samples_init), [n,n]))\n",
    "plt.show()\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reduce_mean(modelo_samples, 0, )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import time\n",
    "import tensorflow as tf\n",
    "import tensorflow_probability as tfp\n",
    "\n",
    "\n",
    "n_ensemble = 30\n",
    "n_samples = 100\n",
    "cut_factor = 1\n",
    "\n",
    "#plt.figure(dpi=120)\n",
    "#plt.plot(wavelet)\n",
    "#plt.show()\n",
    "\n",
    "latent_dim = cvae.latent_dim\n",
    "\n",
    "G = tf.constant(acoustic_foward_matrix(wavelet,I), np.float32)\n",
    "\n",
    "plt.figure(dpi=120)\n",
    "plt.imshow(G)\n",
    "plt.show()\n",
    "\n",
    "modelo_samples = tf.Variable(tf.random.normal(shape=[n_samples, cvae.latent_dim, cvae.latent_dim, 1]))\n",
    "sismica_samples = tf.Variable(tf.zeros([n_samples, G.shape[0], G.shape[1]]))\n",
    "imp_samples =  tf.Variable(tf.zeros([n_samples, G.shape[0]+1, G.shape[1]]))\n",
    "\n",
    "samples_init = tf.random.normal([1, cvae.latent_dim, cvae.latent_dim, 1])\n",
    "gan_facies = tf.experimental.numpy.around(cvae.sample(eps=samples_init)*3)\n",
    "\n",
    "plt.imshow(gan_facies.reshape(n,n))\n",
    "plt.show()\n",
    "plt.close()\n",
    "\n",
    "#fig = plt.figure(figsize=(10, 10))\n",
    "#for i in range(16):\n",
    "#   plt.subplot(4, 4, i + 1)\n",
    "#   plt.imshow(np.around(gan_facies[i, :, :, 0]))\n",
    "#   plt.axis('off')\n",
    "#plt.show()\n",
    "modelo_referencia = tf.constant(ss.reshape(1,n,n,1), dtype=tf.float32)\n",
    "modelo_referencia_latent = tf.constant(cvae.encode(modelo_referencia/3)[0])\n",
    "sismica_experimental_tmp, imp_experimental_tmp = facies_forward_model_2D(modelo_referencia.numpy().reshape(n,n),PRIOR,G.numpy(), v_fact)\n",
    "sismica_experimental = tf.constant(sismica_experimental_tmp.reshape(-1,J), dtype=tf.float32)\n",
    "imp_experimental = tf.constant(imp_experimental_tmp.reshape(I,J), dtype=tf.float32)\n",
    "\n",
    "noise = np.random.randn(I-1,J)\n",
    "noise = noise/np.std(noise)\n",
    "std_noise = np.std(sismica_experimental)/np.sqrt(signal2noise)\n",
    "noise = noise*std_noise\n",
    "sismica_experimental = sismica_experimental + noise\n",
    "\n",
    "fig, axs = plt.subplots(1,3)\n",
    "fig.set_dpi(120)\n",
    "axs[0].imshow(modelo_referencia.numpy().reshape(n,n))\n",
    "axs[1].imshow(sismica_experimental, cmap='gray')\n",
    "axs[2].imshow(imp_experimental)\n",
    "plt.margins(0,0)\n",
    "plt.show()\n",
    "\n",
    "C_d = std_noise**2*tf.eye(sismica_experimental.shape[0]*sismica_experimental.shape[1])\n",
    "plt.imshow(C_d)\n",
    "plt.show()\n",
    "\n",
    "inflation_factor = np.linspace(10,1,n_ensemble)\n",
    "c = np.sum(1/inflation_factor)\n",
    "inflation_factor = tf.constant(c*inflation_factor, dtype=tf.float32)\n",
    "#inflation_factor = tf.ones([n_ensemble,1])*10\n",
    "\n",
    "erro_referencia_samples = tf.Variable(tf.zeros([n_samples]))\n",
    "erro_referencia_samples_latent = tf.Variable(tf.zeros([n_samples]))\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "for sample in range(0,n_samples):\n",
    "    gan_facies = tf.experimental.numpy.around(cvae.sample(eps=modelo_samples[sample:sample+1])*3)\n",
    "\n",
    "    sismica_experimental_tmp, imp_experimental_tmp = facies_forward_model_2D(np.array(gan_facies.reshape(n,n)),PRIOR,G.numpy(), v_fact)\n",
    "    imp_samples[sample].assign(imp_experimental_tmp)\n",
    "    sismica_samples[sample].assign(sismica_experimental_tmp)\n",
    "    erro_referencia_samples[sample].assign(tf.reduce_mean(tf.math.squared_difference(modelo_referencia/3, gan_facies/3)))\n",
    "    erro_referencia_samples_latent[sample].assign(tf.reduce_mean(tf.math.squared_difference(modelo_referencia_latent, modelo_samples[sample])))\n",
    "\n",
    "print('Total init time(s): ', time.time() - start_time)\n",
    "\n",
    "plt_ensamble(modelo_samples)\n",
    "\n",
    "plt.hist(tf.reduce_mean(modelo_samples,0).numpy().flatten())\n",
    "plt.title('Media modelo_samples')\n",
    "plt.show()\n",
    "\n",
    "media_ensemble = tf.Variable(tf.zeros([n_ensemble, 1, cvae.latent_dim, cvae.latent_dim, 1]))\n",
    "erro_sismico = tf.Variable(tf.zeros([n_ensemble-1]))\n",
    "erro_impedance = tf.Variable(tf.zeros([n_ensemble-1]))\n",
    "erro_referencia = tf.Variable(tf.zeros([n_ensemble-1]))\n",
    "erro_referencia_latent = tf.Variable(tf.zeros([n_ensemble-1]))\n",
    "\n",
    "for ensemble in range (1,n_ensemble):\n",
    "    start_time_ensemble = time.time()\n",
    "    erro_sismico[ensemble-1].assign(tf.reduce_mean(tf.math.squared_difference(sismica_experimental, tf.reduce_mean(sismica_samples,0))))\n",
    "    erro_impedance[ensemble-1].assign(tf.reduce_mean(tf.math.squared_difference(imp_experimental, tf.reduce_mean(imp_samples,0))))\n",
    "    erro_referencia[ensemble-1].assign(tf.reduce_mean(erro_referencia_samples))\n",
    "    erro_referencia_latent[ensemble-1].assign(tf.reduce_mean(erro_referencia_samples_latent))\n",
    "    \n",
    "    media_ensemble[ensemble-1].assign(tf.reduce_mean(modelo_samples, 0, keepdims=True))\n",
    "\n",
    "    \n",
    "\n",
    "    #U_t = tf.transpose(tf.linalg.cholesky(C_d))\n",
    "    #noise_t = tf.random.normal([U_t.shape[0],1], mean=0.0, stddev=1.0)\n",
    "    #d_tio = tf.add(sismica_experimental, tf.reshape(tf.math.sqrt(inflation_factor[ensemble])*tf.matmul(U_t, noise_t), sismica_experimental.shape))\n",
    "\n",
    "    d_tio = tf.add(sismica_experimental, tf.math.sqrt(inflation_factor[ensemble])*tf.random.normal(sismica_experimental.shape)*std_noise)\n",
    "\n",
    "    fig, axs = plt.subplots(1,2, figsize=(10, 6))\n",
    "    axs[0].imshow(d_tio)\n",
    "    axs[0].set_title('d_tio')\n",
    "    axs[1].imshow(sismica_experimental)\n",
    "    axs[1].set_title('d')\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "    data_diff = tf.reshape(tf.transpose(sismica_samples - tf.reduce_mean(sismica_samples), perm=[0,2,1]),[n_samples,-1])\n",
    "    #model_diff = tf.reshape(tf.transpose(modelo_samples - tf.reduce_mean(modelo_samples, 0), perm=[0,1,3,2,4]), [n_samples,-1])\n",
    "    model_diff = tf.reshape(modelo_samples - tf.reduce_mean(modelo_samples), [n_samples,-1])\n",
    "    \n",
    "    #C_dd = tfp.stats.covariance(tf.reshape(tf.transpose(sismica_samples, perm=[0,2,1]),[n_samples,-1]))\n",
    "    C_dd = tf.linalg.matmul(tf.transpose(data_diff), data_diff)/(n_samples-1)\n",
    "    C_md = tf.linalg.matmul(tf.transpose(model_diff), data_diff)/(n_samples-1)\n",
    "\n",
    "    fig = plt.figure(dpi=140)\n",
    "    plt.imshow(C_dd[:200,:200])\n",
    "    plt.show()\n",
    "\n",
    "    fig = plt.figure(dpi=140)\n",
    "    plt.imshow(C_md[:,:200])\n",
    "    plt.show()\n",
    "    #C_dd = tf.linalg.matmul(tf.transpose(data_diff), data_diff)/(n_samples-1)\n",
    "    #C_dd_experimental = tfp.stats.covariance(tf.reshape(sismica_samples,[n_samples,-1]), tf.reshape(sismica_samples,[n_samples,-1]))\n",
    "    #C_d_experimental = tfp.stats.covariance(tf.reshape(sismica_samples, [sismica_samples.shape[0],-1]))\n",
    "    \n",
    "    K = tf.linalg.matmul(C_md, tf.linalg.pinv(C_dd + inflation_factor[ensemble]*C_d, 0.001*tf.reduce_mean(tf.linalg.tensor_diag_part(C_d))))\n",
    "    \n",
    "    #fig = plt.figure(dpi=140)\n",
    "    #fig.colorbar(plt.imshow(K[:,:200]))\n",
    "    #plt.show()\n",
    "    print(C_md.shape)\n",
    "    print(K.shape)\n",
    "\n",
    "    for sample in range(0,n_samples):\n",
    "        #K = tf.reshape(tf.linalg.matmul(cmd_inv,tf.reshape(d_tio-sismica_samples[sample],[-1,1])), modelo_samples[sample].shape)\n",
    "       \n",
    "        #dtio_dp = tf.reshape(tf.transpose(d_tio-sismica_samples[sample], perm=[1,0]),[-1,1])\n",
    "        dtio_dp = tf.reshape(d_tio-sismica_samples[sample], [-1,1])\n",
    "        Ksum = tf.reshape(tf.linalg.matmul(K,dtio_dp), modelo_samples[sample].shape)\n",
    "        rmss = modelo_samples[sample] + Ksum\n",
    "\n",
    "        modelo_samples[sample].assign(rmss)\n",
    "\n",
    "        gan_facies = tf.experimental.numpy.around(cvae.sample(eps=modelo_samples[sample:sample+1])*3)\n",
    "\n",
    "        sismica_experimental_tmp, imp_experimental_tmp = facies_forward_model_2D(np.array(gan_facies.reshape(n,n)),PRIOR,G.numpy(), v_fact)\n",
    "        imp_samples[sample].assign(imp_experimental_tmp)\n",
    "        sismica_samples[sample].assign(sismica_experimental_tmp)\n",
    "        erro_referencia_samples[sample].assign(tf.reduce_mean(tf.math.squared_difference(modelo_referencia/3, gan_facies/3)))\n",
    "        erro_referencia_samples_latent[sample].assign(tf.reduce_mean(tf.math.squared_difference(modelo_referencia_latent, modelo_samples[sample])))\n",
    "\n",
    "    print('Total update time(s): ', time.time() - start_time_ensemble)\n",
    "    plt_ensamble(modelo_samples)\n",
    "\n",
    "    mean_ss = tf.reduce_mean(modelo_samples,0, keepdims=True)\n",
    "    \n",
    "    fig, axs = plt.subplots(1,2, figsize=(10, 6))\n",
    "    axs[0].imshow(tf.reshape(mean_ss, [latent_dim, latent_dim]))\n",
    "    axs[0].set_title('media modelo_samples')\n",
    "    axs[1].hist(mean_ss.numpy().flatten())\n",
    "    axs[1].set_title('hist')\n",
    "    plt.show()\n",
    "\n",
    "    facies_es = tf.reshape(tf.experimental.numpy.around(cvae.sample(eps=mean_ss)*3), [n,n])\n",
    "    sismica_es, imp_es = facies_forward_model_2D(facies_es.numpy(),PRIOR,G.numpy(), v_fact)\n",
    "\n",
    "    fig, axs = plt.subplots(2,3, figsize=(10, 6))\n",
    "    axs[0,0].imshow(tf.reshape(modelo_referencia, [n,n]))\n",
    "    axs[0,1].imshow(tf.reshape(sismica_experimental,[n-1,n]), cmap='gray')\n",
    "    axs[0,2].imshow(imp_experimental)\n",
    "    axs[1,0].imshow(facies_es)\n",
    "    axs[1,1].imshow(sismica_es, cmap='gray')\n",
    "    axs[1,2].imshow(imp_es)\n",
    "    fig.tight_layout()\n",
    "    plt.show()\n",
    "    plt.close()\n",
    "\n",
    "    fig, axs = plt.subplots(1,3, figsize=(10, 6))\n",
    "    axs[0].plot(erro_referencia[:ensemble])\n",
    "    axs[1].plot(erro_sismico[:ensemble])\n",
    "    axs[2].plot(erro_referencia_latent[:ensemble])\n",
    "    plt.show()\n",
    "    plt.close()\n",
    "\n",
    "print('Total time(s): ', time.time() - start_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "C_dd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "C_md.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelo_samples = tf.Variable(np.random.normal(size=(n_samples, 1, cvae.latent_dim, cvae.latent_dim, 1)), dtype=tf.float32)\n",
    "plt.hist(modelo_samples[0].numpy().flatten())\n",
    "plt.show()\n",
    "plt.hist(np.mean(modelo_samples.numpy()).flatten())\n",
    "plt.show()\n",
    "print(np.mean(modelo_samples.numpy()))\n",
    "print(tf.reduce_mean(modelo_samples))\n",
    "plt.hist((modelo_samples - tf.reduce_mean(modelo_samples)).numpy().flatten())\n",
    "plt.show()\n",
    "plt.hist((modelo_samples - tf.reduce_mean(modelo_samples,0)).numpy().flatten())\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(C_md.shape, C_dd.shape)\n",
    "varn=1-1/math.pow(10,2)\n",
    "print(varn)\n",
    "s, u, v = tf.linalg.svd(C_dd + inflation_factor[ensemble]*C_d)\n",
    "print(s)\n",
    "diagonal = s\n",
    "for i in range(len(diagonal)):\n",
    "    if (tf.reduce_sum(diagonal[0:i+1]))/(tf.reduce_sum(diagonal)) > varn:\n",
    "        diagonal = (diagonal[0:i+1])\n",
    "        break\n",
    "\n",
    "print(diagonal)\n",
    "u=u[:,0:i+1]\n",
    "v=v[:,0:i+1]\n",
    "ss = tf.linalg.diag(diagonal**(-1))\n",
    "K=tf.experimental.numpy.dot(C_md,(tf.experimental.numpy.dot(tf.experimental.numpy.dot(v,ss),(tf.transpose(u)))))\n",
    "print(K)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.linalg.pinv(C_dd + inflation_factor[ensemble]*C_d, 0.001*tf.reduce_mean(tf.linalg.diag(C_d)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(0.001*tf.reduce_mean(tf.linalg.tensor_diag_part(C_d)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "K = tf.linalg.matmul(C_md, tf.linalg.inv(C_dd + inflation_factor[ensemble]*C_d))\n",
    "dtio_dp = tf.reshape(tf.transpose(d_tio-sismica_samples[sample], perm=[1,0]),[-1,1])\n",
    "Ksum = tf.reshape(tf.linalg.matmul(K,dtio_dp), modelo_samples[sample].shape)\n",
    "print(Ksum)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(C_d.shape)\n",
    "nnoise = tf.math.sqrt(inflation_factor[ensemble])*tf.random.normal(sismica_experimental.shape)*0.5*std_noise\n",
    "fig = plt.figure(dpi=100)\n",
    "fig.colorbar(plt.imshow(sismica_experimental))\n",
    "plt.show()\n",
    "fig = plt.figure(dpi=100)\n",
    "fig.colorbar(plt.imshow(sismica_experimental+nnoise))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(model_diff.shape)\n",
    "data_diff = (sismica_samples - tf.reduce_mean(sismica_samples,0))\n",
    "print(data_diff.shape)\n",
    "\n",
    "tf.linalg.matmul(tf.transpose(model_diff), tf.reshape(tf.transpose(data_diff, perm=[0,2,1]),[n_samples,-1]))\n",
    "plt.imshow(tf.linalg.matmul(tf.transpose(model_diff), tf.reshape(tf.transpose(data_diff, perm=[0,2,1]),[n_samples,-1]))[:,:200])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(C_d.shape)\n",
    "print(sismica_experimental.shape)\n",
    "R = np.linalg.cholesky(C_d) #Matriz triangular inferior\n",
    "U = R.T   #Matriz R transposta\n",
    "p , w =np.linalg.eig(C_d)\n",
    "\n",
    "aux = tf.repeat(tf.reshape(sismica_experimental,[-1,1]), n_samples, axis=-1)\n",
    "mean = 0*(tf.transpose(tf.reshape(sismica_experimental,[-1,1])))\n",
    "print(mean.shape)\n",
    "nnoise = tf.transpose(np.random.multivariate_normal(mean[0], np.eye(2256), n_samples))\n",
    "dddd = np.dot(U, nnoise)\n",
    "d_obs = aux+math.sqrt(10)*dddd\n",
    "print(d_obs.shape)\n",
    "plt.imshow(aux[:,:100])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(d_obs[:10,:10])\n",
    "print(sismica_experimental[:10,:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(C_dd_experimental.shape)\n",
    "data_diff = sismica_samples - tf.reduce_mean(sismica_samples,0)\n",
    "model_diff = tf.reshape(modelo_samples - media_ensemble[ensemble-1], [-1,6,6])\n",
    "ff = tfp.stats.covariance(sismica_samples, sismica_samples, event_axis=None)#/(n_samples-1)\n",
    "print(ff)\n",
    "ff2 = tfp.stats.covariance(tf.reshape(sismica_samples,[n_samples,-1]), tf.reshape(sismica_samples,[n_samples,-1]))\n",
    "#ff2 = tfp.stats.covariance(tf.reshape(modelo_samples,[n_samples,-1]), tf.reshape(sismica_samples,[n_samples,-1]), event_axis=None)\n",
    "print(ff2)\n",
    "fig = plt.figure(dpi=100)\n",
    "fig.colorbar(plt.imshow(ff2))\n",
    "plt.show()\n",
    "\n",
    "data_diff_2 = tf.reshape((sismica_samples - tf.reduce_mean(sismica_samples,0)), [n_samples,-1])\n",
    "ff3 = tf.linalg.matmul(tf.transpose(data_diff_2), data_diff_2)/(n_samples-1)\n",
    "print(ff3)\n",
    "fig = plt.figure(dpi=100)\n",
    "fig.colorbar(plt.imshow(ff3))\n",
    "plt.show()\n",
    "\n",
    "ff3_ = tf.experimental.numpy.dot(tf.transpose(data_diff_2), data_diff_2)/(n_samples-1)\n",
    "print(tf.reduce_sum(ff3-ff3_))\n",
    "fig = plt.figure(dpi=100)\n",
    "fig.colorbar(plt.imshow(ff3))\n",
    "plt.show()\n",
    "\n",
    "model_diff_2 = tf.reshape(modelo_samples - tf.reduce_mean(modelo_samples,0), [n_samples,-1])\n",
    "ff4 = tf.linalg.matmul(tf.transpose(model_diff_2), data_diff_2)/(n_samples-1)\n",
    "print(ff4)\n",
    "#fig = plt.figure(dpi=100)\n",
    "#fig.colorbar(plt.imshow(ff4[:,:36]))\n",
    "#plt.show()\n",
    "\n",
    "K1 = tf.reshape(tf.linalg.matmul(cmd_inv,tf.reshape(d_tio-sismica_samples[sample],[-1,1])), modelo_samples[sample].shape)\n",
    "K2 = tf.reshape(tf.experimental.numpy.dot(cmd_inv,tf.reshape(d_tio-sismica_samples[sample],[-1])), modelo_samples[sample].shape)\n",
    "print(K1-K2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.repe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(C_d_experimental.shape, C_d_experimental)\n",
    "print(C_d.shape, C_d)\n",
    "print(inflation_factor[ensemble])\n",
    "print(modelo_samples[sample])\n",
    "inv_m = tf.linalg.pinv(C_d_experimental + inflation_factor[ensemble]*C_d)\n",
    "print(inv_m.shape, inv_m)\n",
    "print(C_md_experimental, C_md_experimental.shape)\n",
    "#cmd_inv = tf.linalg.matmul(C_md_experimental,inv_m)\n",
    "cmd_inv = tf.experimental.numpy.dot(inv_m, tf.reshape(d_tio-sismica_samples[sample],[-1]))\n",
    "print(cmd_inv.shape, cmd_inv)\n",
    "K = tf.reshape(tf.experimental.numpy.dot(C_md_experimental,cmd_inv), modelo_samples[sample].shape)\n",
    "print(K.shape, K)\n",
    "rmss = modelo_samples[sample] + K\n",
    "print(rmss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#modelo_samples[sample] = modelo_samples[sample] + C_md_experimental.dot(np.linalg.pinv(C_d_experimental + inflation_factor[ensemble]*C_d)).dot((d_tio-sismica_samples[sample]).reshape(-1))\n",
    "print(C_d_experimental.shape, C_d_experimental)\n",
    "print(C_d.shape, C_d)\n",
    "print(inflation_factor[ensemble])\n",
    "print(modelo_samples[sample])\n",
    "inv_m = np.linalg.pinv(C_d_experimental + inflation_factor[ensemble]*C_d)\n",
    "print(inv_m.shape, inv_m)\n",
    "print(C_md_experimental, C_md_experimental.shape)\n",
    "cmd_inv = inv_m.dot((d_tio-sismica_samples[sample]).reshape(-1))\n",
    "print(cmd_inv.shape, cmd_inv)\n",
    "K = C_md_experimental.dot(cmd_inv)\n",
    "print(K.shape, K)\n",
    "rmss = modelo_samples[sample] + K\n",
    "print(rmss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "C_md_experimental.dot(np.linalg.pinv(C_d_experimental + inflation_factor[ensemble]*C_d)).dot((d_tio-sismica_samples[sample]).reshape(-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "j0aJu_hz8KLa",
    "outputId": "9380531a-197a-409d-d75e-c8f33e80c5e9"
   },
   "outputs": [],
   "source": [
    "import time\n",
    "import tensorflow as tf\n",
    "import tensorflow_probability as tfp\n",
    "\n",
    "\n",
    "n_ensemble = 10\n",
    "n_samples = 25\n",
    "cut_factor = 1\n",
    "\n",
    "#plt.figure(dpi=120)\n",
    "#plt.plot(wavelet)\n",
    "#plt.show()\n",
    "\n",
    "latent_dim = cvae_dense.latent_dim\n",
    "\n",
    "G = tf.constant(acoustic_foward_matrix(wavelet,I), np.float32)\n",
    "\n",
    "plt.figure(dpi=120)\n",
    "plt.imshow(G)\n",
    "plt.show()\n",
    "\n",
    "modelo_samples = tf.Variable(tf.zeros([n_samples, 1, cvae_dense.latent_dim]))\n",
    "sismica_samples = tf.Variable(tf.zeros([n_samples, G.shape[0], G.shape[1]]))\n",
    "imp_samples =  tf.Variable(tf.zeros([n_samples, G.shape[0]+1, G.shape[1]]))\n",
    "\n",
    "samples_init = tf.random.normal([1, cvae_dense.latent_dim])\n",
    "gan_facies = tf.experimental.numpy.around(cvae_dense.sample(eps=samples_init)*3)\n",
    "\n",
    "plt.imshow(gan_facies.reshape(n,n))\n",
    "plt.show()\n",
    "plt.close()\n",
    "\n",
    "#fig = plt.figure(figsize=(10, 10))\n",
    "#for i in range(16):\n",
    "#   plt.subplot(4, 4, i + 1)\n",
    "#   plt.imshow(np.around(gan_facies[i, :, :, 0]))\n",
    "#   plt.axis('off')\n",
    "#plt.show()\n",
    "modelo_referencia = tf.constant(ss.reshape(1,n,n,1), dtype=tf.float32)\n",
    "modelo_referencia_latent = tf.constant(cvae_dense.encode(modelo_referencia/3)[0])\n",
    "sismica_experimental_tmp, imp_experimental_tmp = facies_forward_model_2D(modelo_referencia.numpy().reshape(n,n),PRIOR,G.numpy(), v_fact)\n",
    "sismica_experimental = tf.constant(sismica_experimental_tmp.reshape(-1,J), dtype=tf.float32)\n",
    "imp_experimental = tf.constant(imp_experimental_tmp.reshape(I,J), dtype=tf.float32)\n",
    "\n",
    "#noise = np.random.randn(I-1,J)\n",
    "#noise = noise/np.std(noise)\n",
    "#std_noise = np.std(sismica_experimental)/np.sqrt(signal2noise)\n",
    "#noise = noise*std_noise\n",
    "#sismica_experimental = sismica_experimental + noise\n",
    "\n",
    "fig, axs = plt.subplots(1,3)\n",
    "fig.set_dpi(120)\n",
    "axs[0].imshow(modelo_referencia.numpy().reshape(n,n))\n",
    "axs[1].imshow(sismica_experimental, cmap='gray')\n",
    "axs[2].imshow(imp_experimental)\n",
    "plt.margins(0,0)\n",
    "plt.show()\n",
    "\n",
    "C_d = std_noise**2*tf.eye(sismica_experimental.shape[0]*sismica_experimental.shape[1])\n",
    "plt.imshow(C_d)\n",
    "plt.show()\n",
    "\n",
    "inflation_factor = np.linspace(50,10,n_ensemble)\n",
    "c = np.sum(1/inflation_factor)\n",
    "inflation_factor = tf.constant(c*inflation_factor, dtype=tf.float32)\n",
    "#inflation_factor = tf.ones([n_ensemble,1])*10\n",
    "\n",
    "erro_referencia_samples = tf.Variable(tf.zeros([n_samples]))\n",
    "erro_referencia_samples_latent = tf.Variable(tf.zeros([n_samples]))\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "for sample in range(0,n_samples):\n",
    "    #print(modelo_samples[sample])\n",
    "\n",
    "    modelo_samples[sample].assign(tf.random.normal([1, cvae_dense.latent_dim])*cut_factor)\n",
    "\n",
    "    #print(modelo_samples[sample])\n",
    "\n",
    "    gan_facies = tf.experimental.numpy.around(cvae_dense.sample(eps=modelo_samples[sample])*3)\n",
    "    #print(gan_facies)\n",
    "    #plt.imshow(gan_facies.reshape(n,n))\n",
    "    #plt.show()\n",
    "\n",
    "    sismica_experimental_tmp, imp_experimental_tmp = facies_forward_model_2D(np.array(gan_facies.reshape(n,n)),PRIOR,G.numpy(), v_fact)\n",
    "    imp_samples[sample].assign(imp_experimental_tmp)\n",
    "    sismica_samples[sample].assign(sismica_experimental_tmp)\n",
    "    erro_referencia_samples[sample].assign(tf.reduce_mean(tf.math.squared_difference(modelo_referencia/3, gan_facies/3)))\n",
    "    erro_referencia_samples_latent[sample].assign(tf.reduce_mean(tf.math.squared_difference(modelo_referencia_latent, modelo_samples[sample])))\n",
    "\n",
    "print('Total init time(s): ', time.time() - start_time)\n",
    "\n",
    "plt_ensamble_dense(modelo_samples)\n",
    "\n",
    "plt.hist(tf.reduce_mean(modelo_samples,0).numpy().flatten())\n",
    "plt.show()\n",
    "\n",
    "media_ensemble = tf.Variable(tf.zeros([n_ensemble, 1, cvae_dense.latent_dim]))\n",
    "erro_sismico = tf.Variable(tf.zeros([n_ensemble-1]))\n",
    "erro_impedance = tf.Variable(tf.zeros([n_ensemble-1]))\n",
    "erro_referencia = tf.Variable(tf.zeros([n_ensemble-1]))\n",
    "erro_referencia_latent = tf.Variable(tf.zeros([n_ensemble-1]))\n",
    "\n",
    "for ensemble in range (1,n_ensemble):\n",
    "    start_time_ensemble = time.time()\n",
    "    erro_sismico[ensemble-1].assign(tf.reduce_mean(tf.math.squared_difference(sismica_experimental, tf.reduce_mean(sismica_samples,0))))\n",
    "    erro_impedance[ensemble-1].assign(tf.reduce_mean(tf.math.squared_difference(imp_experimental, tf.reduce_mean(imp_samples,0))))\n",
    "    erro_referencia[ensemble-1].assign(tf.reduce_mean(erro_referencia_samples))\n",
    "    erro_referencia_latent[ensemble-1].assign(tf.reduce_mean(erro_referencia_samples_latent))\n",
    "    \n",
    "    #U_t = tf.transpose(tf.linalg.cholesky(C_d))\n",
    "    #noise_t = tf.random.normal([U_t.shape[0],1], mean=0.0, stddev=1.0)\n",
    "    #d_tio = tf.add(sismica_experimental, tf.reshape(tf.math.sqrt(inflation_factor[ensemble])*tf.matmul(U_t, noise_t), sismica_experimental.shape))\n",
    "\n",
    "    d_tio = tf.add(sismica_experimental, tf.math.sqrt(inflation_factor[ensemble])*tf.random.normal(sismica_experimental.shape)*std_noise)\n",
    "\n",
    "    fig, axs = plt.subplots(1,2, figsize=(10, 6))\n",
    "    axs[0].imshow(d_tio)\n",
    "    axs[0].set_title('d_tio')\n",
    "    axs[1].imshow(sismica_experimental)\n",
    "    axs[1].set_title('d')\n",
    "    plt.show()\n",
    "\n",
    "    media_ensemble[ensemble-1].assign(tf.reduce_mean(modelo_samples, 0))\n",
    "\n",
    "    data_diff = tf.reshape(tf.transpose(sismica_samples - tf.reduce_mean(sismica_samples,0), perm=[0,2,1]),[n_samples,-1])\n",
    "    #model_diff = tf.reshape(tf.transpose(modelo_samples - tf.reduce_mean(modelo_samples, 0), perm=[0,1,3,2,4]), [n_samples,-1])\n",
    "    model_diff = tf.reshape(modelo_samples - tf.reduce_mean(modelo_samples, 0), [n_samples,-1])\n",
    "    \n",
    "    #C_dd = tfp.stats.covariance(tf.reshape(tf.transpose(sismica_samples, perm=[0,2,1]),[n_samples,-1]))\n",
    "    C_dd = tf.linalg.matmul(tf.transpose(data_diff), data_diff)/(n_samples-1)\n",
    "    C_md = tf.linalg.matmul(tf.transpose(model_diff), data_diff)/(n_samples-1)\n",
    "\n",
    "    fig = plt.figure(dpi=140)\n",
    "    plt.imshow(C_dd[:200,:200])\n",
    "    plt.show()\n",
    "\n",
    "    fig = plt.figure(dpi=140)\n",
    "    plt.imshow(C_md[:,:200])\n",
    "    plt.show()\n",
    "    #C_dd = tf.linalg.matmul(tf.transpose(data_diff), data_diff)/(n_samples-1)\n",
    "    #C_dd_experimental = tfp.stats.covariance(tf.reshape(sismica_samples,[n_samples,-1]), tf.reshape(sismica_samples,[n_samples,-1]))\n",
    "    #C_d_experimental = tfp.stats.covariance(tf.reshape(sismica_samples, [sismica_samples.shape[0],-1]))\n",
    "    \n",
    "    K = tf.linalg.matmul(C_md, tf.linalg.pinv(C_dd + inflation_factor[ensemble]*C_d, 0.001*tf.reduce_mean(tf.linalg.tensor_diag_part(C_d))))\n",
    "    \n",
    "    fig = plt.figure(dpi=140)\n",
    "    fig.colorbar(plt.imshow(K[:,:200]))\n",
    "    plt.show()\n",
    "\n",
    "    for sample in range(0,n_samples):\n",
    "        #K = tf.reshape(tf.linalg.matmul(cmd_inv,tf.reshape(d_tio-sismica_samples[sample],[-1,1])), modelo_samples[sample].shape)\n",
    "       \n",
    "        #dtio_dp = tf.reshape(tf.transpose(d_tio-sismica_samples[sample], perm=[1,0]),[-1,1])\n",
    "        dtio_dp = tf.reshape(d_tio-sismica_samples[sample], [-1,1])\n",
    "        Ksum = tf.reshape(tf.linalg.matmul(K,dtio_dp), modelo_samples[sample].shape)\n",
    "        rmss = modelo_samples[sample] + Ksum\n",
    "\n",
    "        modelo_samples[sample].assign(rmss)\n",
    "\n",
    "        gan_facies = tf.experimental.numpy.around(cvae_dense.sample(eps=modelo_samples[sample])*3)\n",
    "\n",
    "        sismica_experimental_tmp, imp_experimental_tmp = facies_forward_model_2D(np.array(gan_facies.reshape(n,n)),PRIOR,G.numpy(), v_fact)\n",
    "        imp_samples[sample].assign(imp_experimental_tmp)\n",
    "        sismica_samples[sample].assign(sismica_experimental_tmp)\n",
    "        erro_referencia_samples[sample].assign(tf.reduce_mean(tf.math.squared_difference(modelo_referencia/3, gan_facies/3)))\n",
    "        erro_referencia_samples_latent[sample].assign(tf.reduce_mean(tf.math.squared_difference(modelo_referencia_latent, modelo_samples[sample])))\n",
    "\n",
    "    print('Total update time(s): ', time.time() - start_time_ensemble)\n",
    "    #facies_es = np.around(cvae.sample(np.mean(modelo_samples,0).reshape(1,cvae.latent_dim))*3).reshape(I,J)\n",
    "    #print(kinv)\n",
    "    plt_ensamble_dense(modelo_samples)\n",
    "\n",
    "    #fig = plt.figure(dpi=100)\n",
    "    #fig.colorbar(plt.imshow(tf.reshape(tf.reduce_mean(modelo_samples,0), [latent_dim])))\n",
    "    #plt.show()\n",
    "\n",
    "    plt.hist(tf.reduce_mean(modelo_samples,0).numpy().flatten())\n",
    "    plt.show()\n",
    "\n",
    "    mean_ss = tf.reduce_mean(modelo_samples,0)\n",
    "    #plt.hist(tf.reshape(modelo_samples,-1))\n",
    "    #plt.show()\n",
    "    #print(tf.reduce_mean(modelo_samples))\n",
    "    facies_es = tf.reshape(tf.experimental.numpy.around(cvae_dense.sample(eps=mean_ss)*3), [n,n])\n",
    "    sismica_es, imp_es = facies_forward_model_2D(facies_es.numpy(),PRIOR,G.numpy(), v_fact)\n",
    "\n",
    "    fig, axs = plt.subplots(2,3, figsize=(10, 6))\n",
    "    axs[0,0].imshow(tf.reshape(modelo_referencia, [n,n]))\n",
    "    axs[0,1].imshow(tf.reshape(sismica_experimental,[n-1,n]), cmap='gray')\n",
    "    axs[0,2].imshow(imp_experimental)\n",
    "    axs[1,0].imshow(facies_es)\n",
    "    axs[1,1].imshow(sismica_es, cmap='gray')\n",
    "    axs[1,2].imshow(imp_es)\n",
    "    fig.tight_layout()\n",
    "    plt.show()\n",
    "    plt.close()\n",
    "\n",
    "    fig, axs = plt.subplots(1,3, figsize=(10, 6))\n",
    "    axs[0].plot(erro_referencia[:ensemble])\n",
    "    axs[1].plot(erro_sismico[:ensemble])\n",
    "    axs[2].plot(erro_referencia_latent[:ensemble])\n",
    "    plt.show()\n",
    "    plt.close()\n",
    "\n",
    "print('Total time(s): ', time.time() - start_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tf.linalg.matmul(C_md, tf.linalg.pinv(C_dd + inflation_factor[ensemble]*C_d, 0.001*tf.reduce_mean(tf.linalg.tensor_diag_part(C_d))))\n",
    "modelo_samples = modelo_samples.numpy().reshape(n_samples,-1).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelo_samples.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from HistoryMatching.ES_MDA import ES_MDA\n",
    "\n",
    "Alpha = np.ones((10),dtype=int)*10\n",
    "for t in range(len(Alpha)):\n",
    "    obs = sismica_experimental.numpy().reshape((-1))\n",
    "    Obs_sim = sismica_samples.numpy().reshape(-1,n_samples)\n",
    "    Obs_sim=  np.array(Obs_sim).T\n",
    "    print('Erro ite_',t, ' : ' ,sum(sum(abs(Obs_sim-obs))))    \n",
    "    m_x = ES_MDA(modelo_samples.shape[1],m_x,obs,Obs_sim,Alpha[t],R,Corr,2)\n",
    "    #m_f = UpdateStateFacies(m_x,dim_shape[0],dim_shape[1],redeVAE)\n",
    "    m_f = redeVAE.predict(m_x)\n",
    "Obs_sim = [((GetFaciesData(m_f[:,i],dim_shape,position,''))) for i in range(m_f.shape[1])]\n",
    "Obs_sim=  np.array(Obs_sim).T\n",
    "print('Erro End: ',sum(sum(abs(Obs_sim-obs))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(tf.reduce_mean(modelo_samples,0).numpy().flatten()/10)\n",
    "plt.show()\n",
    "\n",
    "mean_ss = tf.reduce_mean(modelo_samples,0)/10\n",
    "mean_ss = tf.random.normal([1, cvae_dense.latent_dim])\n",
    "\n",
    "plt.hist(mean_ss.numpy().flatten())\n",
    "plt.show()\n",
    "\n",
    "#print(tf.reduce_mean(modelo_samples))\n",
    "facies_es = tf.reshape(tf.experimental.numpy.around(cvae_dense.sample(eps=mean_ss)*3), [n,n])\n",
    "sismica_es, imp_es = facies_forward_model_2D(facies_es.numpy(),PRIOR,G.numpy(), v_fact)\n",
    "\n",
    "fig, axs = plt.subplots(2,3, figsize=(10, 6))\n",
    "axs[0,0].imshow(tf.reshape(modelo_referencia, [n,n]))\n",
    "axs[0,1].imshow(tf.reshape(sismica_experimental,[n-1,n]), cmap='gray')\n",
    "axs[0,2].imshow(imp_experimental)\n",
    "axs[1,0].imshow(facies_es)\n",
    "axs[1,1].imshow(sismica_es, cmap='gray')\n",
    "axs[1,2].imshow(imp_es)\n",
    "fig.tight_layout()\n",
    "plt.show()\n",
    "plt.close()\n",
    "\n",
    "fig, axs = plt.subplots(1,3, figsize=(10, 6))\n",
    "axs[0].plot(erro_referencia[:ensemble])\n",
    "axs[1].plot(erro_sismico[:ensemble])\n",
    "axs[2].plot(erro_referencia_latent[:ensemble])\n",
    "plt.show()\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tests Fully Conv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "#from cython.parallel cimport prange\n",
    "\n",
    "n = 48\n",
    "n_ensemble = 4\n",
    "n_samples = 36\n",
    "cut_factor = 1\n",
    "\n",
    "#plt.figure(dpi=120)\n",
    "#plt.plot(wavelet)\n",
    "#plt.show()\n",
    "\n",
    "latent_dim = cvae.latent_dim\n",
    "\n",
    "G = acoustic_foward_matrix(wavelet,I)\n",
    "print(G.shape)\n",
    "\n",
    "plt.figure(dpi=120)\n",
    "plt.imshow(G)\n",
    "plt.show()\n",
    "\n",
    "modelo_samples = np.zeros([n_samples, cvae.latent_dim*cvae.latent_dim])\n",
    "\n",
    "sismica_samples = np.zeros([n_samples, G.shape[0], G.shape[1]])\n",
    "\n",
    "imp_samples = np.zeros([n_samples, G.shape[0]+1, G.shape[1]])\n",
    "\n",
    "samples_ini = np.random.normal(size=(1, cvae.latent_dim, cvae.latent_dim, 1))\n",
    "gan_facies = np.around(cvae.sample(samples_ini).numpy()*3)\n",
    "\n",
    "plt.imshow(gan_facies.reshape(n,n))\n",
    "plt.show()\n",
    "\n",
    "modelo_referencia = ss\n",
    "modelo_referencia_latent = cvae.encode(modelo_referencia.reshape(1,modelo_referencia.shape[0], -1, 1)/3)[0].numpy()\n",
    "#print(modelo_referencia_latent)\n",
    "\n",
    "sismica_experimental_tmp, imp_experimental_tmp = facies_forward_model_2D(modelo_referencia,PRIOR,G, v_fact)\n",
    "\n",
    "sismica_experimental = sismica_experimental_tmp.reshape(-1,J)\n",
    "imp_experimental = imp_experimental_tmp.reshape(I,J)\n",
    "\n",
    "#noise = np.random.randn(I-1,J)\n",
    "#noise = noise/np.std(noise)\n",
    "#std_noise = np.std(sismica_experimental)/np.sqrt(signal2noise)\n",
    "#noise = noise*std_noise\n",
    "#sismica_experimental = sismica_experimental + noise\n",
    "\n",
    "fig, axs = plt.subplots(1,3)\n",
    "fig.set_dpi(120)\n",
    "axs[0].imshow(modelo_referencia)\n",
    "axs[1].imshow(sismica_experimental, cmap='gray')\n",
    "axs[2].imshow(imp_experimental)\n",
    "plt.margins(0,0)\n",
    "plt.show()\n",
    "\n",
    "C_d = std_noise**2*np.eye(sismica_experimental.shape[0]*sismica_experimental.shape[1])\n",
    "\n",
    "inflation_factor = np.ones([n_ensemble,1])\n",
    "#inflation_factor = np.linspace(50,10,n_ensemble)\n",
    "c = np.sum(1/inflation_factor)\n",
    "inflation_factor = c*inflation_factor\n",
    "\n",
    "erro_simico_samples = np.zeros([n_samples])\n",
    "erro_referencia_samples = np.zeros([n_samples])\n",
    "erro_referencia_samples_latent = np.zeros([n_samples])\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "for sample in range(0,n_samples):\n",
    "    modelo_samples[sample] = np.random.normal(size=(1, cvae.latent_dim*cvae.latent_dim))*cut_factor\n",
    "\n",
    "    gan_facies = np.around(cvae.sample(modelo_samples[sample].reshape(1,cvae.latent_dim, cvae.latent_dim,1)).numpy()*3).reshape(I,J)\n",
    "\n",
    "    sismica_experimental_tmp, imp_experimental_tmp = facies_forward_model_2D(gan_facies,PRIOR,G, v_fact)\n",
    "    imp_samples[sample] = imp_experimental_tmp\n",
    "    sismica_samples[sample] = sismica_experimental_tmp\n",
    "    erro_simico_samples[sample] = (np.subtract(sismica_experimental, sismica_experimental_tmp)**2).sum()/(n*n)\n",
    "    erro_referencia_samples[sample] = (np.subtract(modelo_referencia, gan_facies)**2).sum()/(n*n)\n",
    "    erro_referencia_samples_latent[sample] = (np.subtract(modelo_samples[sample], modelo_samples[sample].reshape(modelo_referencia_latent.shape))**2).sum()/(cvae.latent_dim**2)\n",
    "\n",
    "print('Total init time(s): ', time.time() - start_time)\n",
    "\n",
    "media_ensemble = np.zeros([n_ensemble, cvae.latent_dim*cvae.latent_dim])\n",
    "erro_sismico = np.zeros([n_ensemble-1])\n",
    "erro_impedance = np.zeros([n_ensemble-1])\n",
    "erro_referencia = np.zeros([n_ensemble-1])\n",
    "erro_referencia_latent = np.zeros([n_ensemble-1])\n",
    "\n",
    "plt_ensamble(modelo_samples.reshape([n_samples, 1, cvae.latent_dim, cvae.latent_dim, 1]))\n",
    "\n",
    "for ensemble in range (1,n_ensemble):\n",
    "    start_time_ensemble = time.time()\n",
    "    \n",
    "    media_ensemble[ensemble-1] = np.mean(modelo_samples)\n",
    "\n",
    "    d_tio = sismica_experimental + np.sqrt(inflation_factor[ensemble])*np.random.randn(sismica_experimental.shape[0], sismica_experimental.shape[1])*std_noise\n",
    "\n",
    "    erro_sismico[ensemble-1] = np.mean(erro_simico_samples)\n",
    "\n",
    "    erro_impedance[ensemble-1] = (np.subtract(imp_experimental, np.mean(imp_samples))**2).sum()/(n*n)\n",
    "\n",
    "    erro_referencia[ensemble-1] = np.mean(erro_referencia_samples)\n",
    "\n",
    "    erro_referencia_latent[ensemble-1] = np.mean(erro_referencia_samples_latent)\n",
    "\n",
    "    C_d_experimental = np.cov(np.transpose(sismica_samples,axes=(0,2,1)).reshape([sismica_samples.shape[0], -1]),rowvar=False)\n",
    "    fig = plt.figure(dpi=100)\n",
    "    fig.colorbar(plt.imshow(C_d_experimental[:200,:200]))\n",
    "    plt.title('Cd')\n",
    "    plt.show()\n",
    "    \n",
    "    C_md_experimental = (modelo_samples - media_ensemble[ensemble-1]).transpose().dot((sismica_samples - np.mean(sismica_samples,0)).reshape(sismica_samples.shape[0],-1))/(n_samples-1)\n",
    "    fig = plt.figure(dpi=100)\n",
    "    fig.colorbar(plt.imshow(C_md_experimental.reshape(36,-1)))\n",
    "    plt.title('Cmd')\n",
    "    plt.show()\n",
    "    print((modelo_samples - media_ensemble[ensemble-1]).transpose().shape)\n",
    "    print((sismica_samples - np.mean(sismica_samples,0)).reshape(sismica_samples.shape[0],-1).shape)\n",
    "\n",
    "    for sample in range(0,n_samples):\n",
    "\n",
    "        modelo_samples[sample] = modelo_samples[sample] + C_md_experimental.dot(np.linalg.inv(C_d_experimental + inflation_factor[ensemble]*C_d)).dot((d_tio-sismica_samples[sample]).reshape(-1))\n",
    "\n",
    "        gan_facies = np.around(cvae.sample(modelo_samples[sample].reshape(1,cvae.latent_dim, cvae.latent_dim, 1)).numpy()*3).reshape(I,J)\n",
    "\n",
    "        sismica_experimental_tmp, imp_experimental_tmp = facies_forward_model_2D(gan_facies,PRIOR,G, v_fact)\n",
    "        imp_samples[sample] = imp_experimental_tmp\n",
    "        sismica_samples[sample] = sismica_experimental_tmp\n",
    "        erro_simico_samples[sample] = (np.subtract(sismica_experimental, sismica_experimental_tmp)**2).sum()/(n*n)\n",
    "        erro_referencia_samples[sample] = (np.subtract(modelo_referencia, gan_facies)**2).sum()/(n*n)\n",
    "        erro_referencia_samples_latent[sample] = (np.subtract(modelo_samples[sample], modelo_samples[sample].reshape(modelo_referencia_latent.shape))**2).sum()/(cvae.latent_dim**2)\n",
    "\n",
    "    plt_ensamble(modelo_samples.reshape([n_samples, 1, cvae.latent_dim, cvae.latent_dim, 1]))\n",
    "    facies_es = np.around(cvae.sample(np.mean(modelo_samples,0).reshape(1,cvae.latent_dim, cvae.latent_dim, 1))*3).reshape(I,J)\n",
    "\n",
    "    fig, axs = plt.subplots(2,3, figsize=(10, 6))\n",
    "    axs[0,0].imshow(modelo_referencia)\n",
    "    axs[0,1].imshow(sismica_experimental, cmap='gray')\n",
    "    axs[0,2].imshow(imp_experimental)\n",
    "    axs[1,0].imshow(facies_es)\n",
    "    axs[1,1].imshow(np.mean(sismica_samples,0), cmap='gray')\n",
    "    axs[1,2].imshow(np.mean(imp_samples,0))\n",
    "    fig.tight_layout()\n",
    "    plt.show()\n",
    "    plt.close()\n",
    "\n",
    "    fig, axs = plt.subplots(1,3, figsize=(10, 6))\n",
    "    axs[0].plot(erro_referencia[1:ensemble+1])\n",
    "    axs[1].plot(erro_sismico[1:ensemble+1])\n",
    "    axs[2].plot(erro_referencia_latent[1:ensemble+1])\n",
    "    plt.show()\n",
    "    plt.close()\n",
    "\n",
    "print('Total time(s): ', time.time() - start_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=[20,20])\n",
    "fig.colorbar(plt.imshow(sismica_samples[10]))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(sismica_samples.shape)\n",
    "print(np.transpose(sismica_samples,axes=(0,2,1)).shape)\n",
    "fig = plt.figure(figsize=[20,20])\n",
    "fig.colorbar(plt.imshow(np.cov(np.transpose(sismica_samples,axes=(0,2,1)).reshape([sismica_samples.shape[0], -1]),rowvar=False)[:200,:200]))\n",
    "plt.show()\n",
    "\n",
    "print((sismica_samples - np.mean(sismica_samples,0)).shape)\n",
    "#tmp = (modelo_samples - media_ensemble[ensemble-1]).dot((sismica_samples - np.mean(sismica_samples,0)).reshape(sismica_samples.shape[0],-1).transpose())\n",
    "tmp = np.cov((modelo_samples - media_ensemble[ensemble-1]), np.transpose(sismica_samples - np.mean(sismica_samples,0),axes=(0,2,1)).reshape(sismica_samples.shape[0],-1))\n",
    "fig = plt.figure(figsize=[20,20])\n",
    "plt.imshow(tmp)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 231
    },
    "id": "nAV4zMFQkDLv",
    "outputId": "73c22a77-21cf-4287-cad6-1d39c8dc467c"
   },
   "outputs": [],
   "source": [
    "print(sismica_samples.shape)\n",
    "print(np.mean(sismica_samples,0).shape)\n",
    "fig = plt.figure(dpi=100)\n",
    "fig.colorbar(plt.imshow(sismica_samples[0]))\n",
    "plt.show()\n",
    "fig = plt.figure(dpi=100)\n",
    "fig.colorbar(plt.imshow(np.mean(sismica_samples,0)))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 772
    },
    "id": "ZZFa41AnjdEH",
    "outputId": "6bfd38e6-851d-422b-9dfa-0346aef08d68"
   },
   "outputs": [],
   "source": [
    "plt.plot(erro_sismico[:-1])\n",
    "plt.show()\n",
    "plt.plot(erro_referencia[:-1])\n",
    "plt.show()\n",
    "plt.plot(erro_referencia_samples[:-1])\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "G8v7S3E4BoJb"
   },
   "source": [
    "# Ensemble Smoother"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 231
    },
    "id": "XzJiPcSB9N0O",
    "outputId": "ca7be55c-ff04-4c3f-9014-d5b9ac5a18e8"
   },
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "n_ensemble = 20\n",
    "n_samples = 100\n",
    "cut_factor = 1\n",
    "\n",
    "#plt.figure(dpi=120)\n",
    "#plt.plot(wavelet)\n",
    "#plt.show()\n",
    "\n",
    "latent_dim = wgan.latent_dim\n",
    "\n",
    "G = acoustic_foward_matrix(wavelet,I)\n",
    "print(G.shape)\n",
    "\n",
    "plt.figure(dpi=120)\n",
    "plt.imshow(G)\n",
    "plt.show()\n",
    "\n",
    "modelo_samples = np.zeros([n_samples, wgan.latent_dim*wgan.latent_dim])\n",
    "\n",
    "sismica_samples = np.zeros([n_samples, G.shape[0], G.shape[1]])\n",
    "\n",
    "imp_samples = np.zeros([n_samples, G.shape[0]+1, G.shape[1]])\n",
    "\n",
    "samples_ini = np.random.normal(size=(1, wgan.latent_dim*wgan.latent_dim))\n",
    "gan_facies = np.around((0.5*wgan.generator.predict(samples_ini.reshape(1, wgan.latent_dim, wgan.latent_dim, 1))+0.5011)*3)\n",
    "modelo_referencia = ss\n",
    "\n",
    "sismica_experimental_tmp, imp_experimental_tmp = facies_forward_model_2D(modelo_referencia,PRIOR,G, v_fact)\n",
    "\n",
    "sismica_experimental = sismica_experimental_tmp.reshape(-1,J)\n",
    "imp_experimental = imp_experimental_tmp.reshape(I,J)\n",
    "\n",
    "#noise = np.random.randn(I-1,J)\n",
    "#noise = noise/np.std(noise)\n",
    "#std_noise = np.std(sismica_experimental)/np.sqrt(signal2noise)\n",
    "#noise = noise*std_noise\n",
    "#sismica_experimental = sismica_experimental + noise\n",
    "\n",
    "fig, axs = plt.subplots(1,3)\n",
    "fig.set_dpi(120)\n",
    "axs[0].imshow(modelo_referencia)\n",
    "axs[1].imshow(sismica_experimental, cmap='gray')\n",
    "axs[2].imshow(imp_experimental)\n",
    "plt.margins(0,0)\n",
    "plt.show()\n",
    "\n",
    "C_d = std_noise**2*np.eye(sismica_experimental.shape[0]*sismica_experimental.shape[1])\n",
    "\n",
    "inflation_factor = np.ones([n_ensemble,1])\n",
    "#inflation_factor = np.linspace(50,10,n_ensemble)\n",
    "c = np.sum(1/inflation_factor)\n",
    "inflation_factor = c*inflation_factor\n",
    "\n",
    "erro_referencia_samples = np.zeros([n_samples])\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "for sample in range(0,n_samples):\n",
    "    modelo_samples[sample] = np.random.normal(size=(1, wgan.latent_dim*wgan.latent_dim)).reshape(-1)*cut_factor\n",
    "\n",
    "    gan_facies = np.around((0.5*wgan.generator.predict(modelo_samples[sample].reshape(1,wgan.latent_dim, -1,1))+0.501)*3).reshape(I,J)\n",
    "\n",
    "    sismica_experimental_tmp, imp_experimental_tmp = facies_forward_model_2D(gan_facies,PRIOR,G, v_fact)\n",
    "    imp_samples[sample] = imp_experimental_tmp\n",
    "    sismica_samples[sample] = sismica_experimental_tmp\n",
    "    erro_referencia_samples[sample] = (np.subtract(modelo_referencia, gan_facies)**2).sum()/n\n",
    "\n",
    "media_ensemble = np.zeros([n_ensemble, modelo_samples.shape[1]])\n",
    "erro_sismico = np.zeros([n_ensemble])\n",
    "erro_impedance = np.zeros([n_ensemble])\n",
    "erro_referencia = np.zeros([n_ensemble])\n",
    "\n",
    "for ensemble in range (1,n_ensemble):\n",
    "    start_time_ensemble = time.time()\n",
    "    \n",
    "    media_ensemble[ensemble-1] = np.mean(modelo_samples)\n",
    "\n",
    "    d_tio = sismica_experimental + np.sqrt(inflation_factor[ensemble])*np.random.randn(sismica_experimental.shape[0], sismica_experimental.shape[1])*std_noise\n",
    "\n",
    "    erro_sismico[ensemble-1] = (np.subtract(sismica_experimental, np.mean(sismica_samples))**2).sum()\n",
    "\n",
    "    erro_impedance[ensemble-1] = (np.subtract(imp_experimental, np.mean(imp_samples))**2).sum()\n",
    "\n",
    "    erro_referencia[ensemble-1] = np.mean(erro_referencia_samples)\n",
    "\n",
    "    C_d_experimental = np.cov(sismica_samples.reshape([sismica_samples.shape[0], -1]).transpose())\n",
    "    fig = plt.figure(dpi=100)\n",
    "    fig.colorbar(plt.imshow(C_d_experimental))\n",
    "    plt.title('Cd')\n",
    "    plt.show()\n",
    "    C_md_experimental = (modelo_samples - media_ensemble[ensemble-1]).transpose().dot((sismica_samples - np.mean(sismica_samples,0)).reshape(sismica_samples.shape[0],-1))/(n_samples-1)\n",
    "\n",
    "    fig = plt.figure(dpi=100)\n",
    "    fig.colorbar(plt.imshow(C_md_experimental))\n",
    "    plt.title('Cmd')\n",
    "    plt.show()\n",
    "\n",
    "    for sample in range(1,n_samples):\n",
    "\n",
    "        modelo_samples[sample] = modelo_samples[sample] + C_md_experimental.dot(np.linalg.pinv(C_d_experimental + inflation_factor[ensemble]*C_d)).dot((d_tio-sismica_samples[sample]).reshape(-1))\n",
    "\n",
    "        gan_facies = np.around((0.5*wgan.generator.predict(modelo_samples[sample].reshape(1,wgan.latent_dim, wgan.latent_dim,1))+0.501)*3).reshape(I,J)\n",
    "\n",
    "        sismica_experimental_tmp, imp_experimental_tmp = facies_forward_model_2D(gan_facies,PRIOR,G, v_fact)\n",
    "        imp_samples[sample] = imp_experimental_tmp\n",
    "        sismica_samples[sample] = sismica_experimental_tmp\n",
    "        erro_referencia_samples[sample] = (np.subtract(modelo_referencia, gan_facies)**2).sum()/n\n",
    "\n",
    "    facies_es = np.around((0.5*wgan.generator.predict(np.mean(modelo_samples,0).reshape(1,wgan.latent_dim, wgan.latent_dim,1))+0.501)*3).reshape(I,J)\n",
    "\n",
    "    fig, axs = plt.subplots(2,3, figsize=(10, 6))\n",
    "    axs[0,0].imshow(modelo_referencia)\n",
    "    axs[0,1].imshow(sismica_experimental)\n",
    "    axs[0,2].imshow(imp_experimental)\n",
    "    axs[1,0].imshow(facies_es)\n",
    "    axs[1,1].imshow(np.mean(sismica_samples,0))\n",
    "    axs[1,2].imshow(np.mean(imp_samples,0))\n",
    "    fig.tight_layout()\n",
    "\n",
    "print('Total time(s): ', time.time() - start_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 441
    },
    "id": "Rpg9h_--xJPK",
    "outputId": "47e8c3f6-baff-4e96-eca0-e8ec259bfb06"
   },
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(2,3, figsize=(10, 6))\n",
    "axs[0,0].imshow(modelo_referencia)\n",
    "axs[0,1].imshow(sismica_experimental)\n",
    "axs[0,2].imshow(imp_experimental)\n",
    "axs[1,0].imshow(facies_es)\n",
    "axs[1,1].imshow(np.mean(sismica_samples,0))\n",
    "axs[1,2].imshow(np.mean(imp_samples,0))\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 231
    },
    "id": "ebVrVr6oAj5W",
    "outputId": "24f8aec7-af16-4e9b-c21a-43408cf53543"
   },
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(1,6)\n",
    "#fig.set_size_inches(7,8)\n",
    "fig.set_dpi(150)\n",
    "fig.tight_layout(pad=0)\n",
    "axs[0].imshow(modelo_referencia)\n",
    "axs[0].axis('off')\n",
    "axs[1].imshow(facies_es)\n",
    "axs[1].axis('off')\n",
    "axs[2].imshow(sismica_experimental)\n",
    "axs[2].axis('off')\n",
    "axs[3].imshow(np.mean(sismica_samples,0))\n",
    "axs[3].axis('off')\n",
    "axs[4].imshow(imp_experimental)\n",
    "axs[4].axis('off')\n",
    "axs[5].imshow(np.mean(imp_samples,0))\n",
    "axs[5].axis('off')\n",
    "\n",
    "#axs[0].set_title('True')\n",
    "#axs[1].set_title('Mean ES')\n",
    "#axs[2].set_title('True Seis')\n",
    "#axs[3].set_title('')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SV0gU2G1Udgm"
   },
   "outputs": [],
   "source": [
    "facies_es = np.around((0.5*wgan.generator.predict(np.mean(modelo_samples,0).reshape(1,-1,1))+0.501)*3).reshape(I,J)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Gh3DrRNhfLI7",
    "outputId": "c83d32ed-95d5-41d2-a40a-7eacc361bac0"
   },
   "outputs": [],
   "source": [
    " (C_md_experimental.dot(np.linalg.pinv(C_d_experimental + inflation_factor[ensemble]*C_d)).dot((d_tio-sismica_samples[sample]).reshape(-1))).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 163
    },
    "id": "G0buvy0RMbT7",
    "outputId": "dffe9844-e8ed-40c2-8991-d6af04abd72b"
   },
   "outputs": [],
   "source": [
    "modelo_samples[sample] + C_md_experimental.transpose().dot(np.linalg.pinv(C_d_experimental + inflation_factor[ensemble]*C_d)).transpose().dot(d_tio-sismica_samples[sample])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 163
    },
    "id": "06w1fWZkNax9",
    "outputId": "ea060f30-f916-4ac4-cbfc-60dd90bd433e"
   },
   "outputs": [],
   "source": [
    "(C_d_experimental + inflation_factor[ensemble]*C_d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "PEYhP941NqCk",
    "outputId": "9b23b0ad-3469-4ba2-f9f2-f4bf9f3c5393"
   },
   "outputs": [],
   "source": [
    "(modelo_samples - media_ensemble[ensemble-1]).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "MOjq1xoZe13U",
    "outputId": "10dba445-9fd4-403f-f098-1e57c922116b"
   },
   "outputs": [],
   "source": [
    "np.mean(sismica_samples,0).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 607
    },
    "id": "n1aIJnvku36R",
    "outputId": "0ca684e9-e75f-4e1e-8179-22d92adaf15d"
   },
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(4)\n",
    "fig.set_dpi(150)\n",
    "fig.tight_layout(pad=1.5)\n",
    "axs[0].plot(erro_sismico_8[:-1].flatten(), label='8')\n",
    "axs[0].plot(erro_sismico_16[:-1].flatten(), label='16')\n",
    "axs[0].plot(erro_sismico_32[:-1].flatten(), label='32')\n",
    "axs[0].legend(fontsize=4,loc='lower left')\n",
    "\n",
    "axs[1].plot(erro_impedance_8[:-1].flatten(), label='8')\n",
    "axs[1].plot(erro_impedance_16[:-1].flatten(), label='16')\n",
    "axs[1].plot(erro_impedance_32[:-1].flatten(), label='32')\n",
    "axs[1].legend(fontsize=4,loc='lower left')\n",
    "\n",
    "axs[2].plot(erro_referencia_8[:-1].flatten(), label='8')\n",
    "axs[2].plot(erro_referencia_16[:-1].flatten(), label='16')\n",
    "axs[2].plot(erro_referencia_32[:-1].flatten(), label='32')\n",
    "axs[2].legend(fontsize=4,loc='lower left')\n",
    "\n",
    "axs[3].plot(inflation_factor.reshape(-1)[:-1].flatten())\n",
    "axs[0].set_title('Seismic Error')\n",
    "axs[1].set_title('Impedance Error')\n",
    "axs[2].set_title('Reference Model Error')\n",
    "axs[3].set_title('Inflation Factor')\n",
    "plt.margins(0,1)\n",
    "#plt.savefig('drive/My Drive/Codes/ensemble_smoother/erros_ml_32_2.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "cVIpl_A9oNXi",
    "outputId": "ae0cae79-e298-44b5-fce6-2b0cca6534d5"
   },
   "outputs": [],
   "source": [
    "print(modelo_samples.shape)\n",
    "all_ensemble = np.around((0.5*gan.generator.predict(modelo_samples.reshape(modelo_samples.shape[1],modelo_samples.shape[0],1))+0.501)*3)\n",
    "plt.figure(dpi=120)\n",
    "plt.imshow(all_ensemble.reshape(all_ensemble.shape[0:-1])[:,:], aspect='auto')\n",
    "plt.show()\n",
    "print(all_ensemble.shape)\n",
    "for i in range(0,n_samples):\n",
    "  all_ensemble[i] = np.around((0.5*gan.generator.predict(modelo_samples[:,i].reshape(1,-1,1))+0.501)*3).reshape(-1,1)\n",
    "gan_facies = np.around((0.5*gan.generator.predict(modelo_samples[:,0].reshape(1,-1,1))+0.501)*3).reshape(-1,1)\n",
    "print(gan_facies.shape)\n",
    "plt.figure(figsize=(3,6))\n",
    "plt.imshow(gan_facies, aspect='auto')\n",
    "plt.show()\n",
    "plt.figure(figsize=(3,6))\n",
    "plt.imshow(all_ensemble[0], aspect='auto')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "id": "hhmJAZ0Uql9t",
    "outputId": "f65fc77a-c3fc-4bb1-f699-c3b19110ff32"
   },
   "outputs": [],
   "source": [
    "print(all_ensemble_8.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "60XuG2_J_IRu",
    "outputId": "b8d5c6ba-3867-48a5-f4aa-5bf58466e1f7"
   },
   "outputs": [],
   "source": [
    "all_ensemble_8 = np.ones([n_samples, n, 1])\n",
    "all_ensemble_16 = np.ones([n_samples, n, 1])\n",
    "all_ensemble_32 = np.ones([n_samples, n, 1])\n",
    "\n",
    "for i in range(0,n_samples):\n",
    "  all_ensemble_8[i] = np.around((0.5*gan8.predict(modelo_samples_8[:,i].reshape(1,-1,1))+0.501)*3).reshape(-1,1)\n",
    "  all_ensemble_16[i] = np.around((0.5*gan16.predict(modelo_samples_16[:,i].reshape(1,-1,1))+0.501)*3).reshape(-1,1)\n",
    "  all_ensemble_32[i] = np.around((0.5*gan32.predict(modelo_samples_32[:,i].reshape(1,-1,1))+0.501)*3).reshape(-1,1)\n",
    "\n",
    "all_ensemble_facies_8 = all_ensemble_8.reshape(all_ensemble_8.shape[0:-1]).transpose()\n",
    "all_ensemble_facies_16 = all_ensemble_16.reshape(all_ensemble_16.shape[0:-1]).transpose()\n",
    "all_ensemble_facies_32 = all_ensemble_32.reshape(all_ensemble_32.shape[0:-1]).transpose()\n",
    "\n",
    "hist_facies_8 = np.zeros([all_ensemble_facies_8.shape[0],1])\n",
    "hist_facies_16 = np.zeros([all_ensemble_facies_16.shape[0],1])\n",
    "hist_facies_32 = np.zeros([all_ensemble_facies_32.shape[0],1])\n",
    "\n",
    "hist_imp_8 = np.zeros([all_ensemble_facies_8.shape[0],50])\n",
    "hist_imp_16 = np.zeros([all_ensemble_facies_16.shape[0],50])\n",
    "hist_imp_32 = np.zeros([all_ensemble_facies_32.shape[0],50])\n",
    "\n",
    "shift_bin_8 = np.zeros([all_ensemble_facies_8.shape[0],51])\n",
    "shift_bin_16 = np.zeros([all_ensemble_facies_16.shape[0],51])\n",
    "shift_bin_32 = np.zeros([all_ensemble_facies_32.shape[0],51])\n",
    "\n",
    "for i in range(0,all_ensemble_facies_32.shape[0]):\n",
    "  ftmp = np.histogram(all_ensemble_facies_8[i,:], bins=[0, 1, 2, 3, 4])[0]\n",
    "  hist_facies_8[i] = np.argmax(ftmp)\n",
    "  hist_imp_8[i,:] = np.histogram(imp_samples_8[i,:], bins=50, range=[9,10])[0]\n",
    "  shift_bin_8[i,:] = np.histogram(imp_samples_8[i,:], bins=50)[1]\n",
    "  \n",
    "  ftmp = np.histogram(all_ensemble_facies_16[i,:], bins=[0, 1, 2, 3, 4])[0]\n",
    "  hist_facies_16[i] = np.argmax(ftmp)\n",
    "  hist_imp_16[i,:] = np.histogram(imp_samples_16[i,:], bins=50, range=[9,10])[0]\n",
    "  shift_bin_16[i,:] = np.histogram(imp_samples_16[i,:], bins=50)[1]\n",
    "\n",
    "  ftmp = np.histogram(all_ensemble_facies_32[i,:], bins=[0, 1, 2, 3, 4])[0]\n",
    "  hist_facies_32[i] = np.argmax(ftmp)\n",
    "  hist_imp_32[i,:] = np.histogram(imp_samples_32[i,:], bins=50, range=[9,10])[0]\n",
    "  shift_bin_32[i,:] = np.histogram(imp_samples_32[i,:], bins=50)[1]\n",
    "\n",
    "plt.figure(dpi=100, figsize=(4,10))\n",
    "plt.imshow(hist_facies_8, aspect='auto')\n",
    "plt.show()\n",
    "\n",
    "#plt.figure(dpi=100, figsize=(4,10))\n",
    "#plt.imshow(hist_imp, aspect='auto')\n",
    "#plt.autoscale(False)\n",
    "#plt.plot(np.flip(imp_experimental),np.linspace(0,128,128))\n",
    "#plt.imshow(hist_imp, aspect='auto')\n",
    "#plt.plot(np.flip(np.mean(imp_samples,1)),np.linspace(0,128,128))\n",
    "#plt.autoscale(False)\n",
    "#plt.show()\n",
    "\n",
    "fig, ax_temp = plt.subplots()\n",
    "fig.set_size_inches((4,20))\n",
    "ax_temp.imshow(np.flipud(hist_imp_8), aspect='auto', extent=[shift_bin_8.min(),shift_bin_8.max(),0,n], origin='lower')\n",
    "print(ax_temp.get_xlim())\n",
    "ax_temp.plot(np.flip(np.mean(imp_samples_8,1)),np.linspace(0,n,n),'r', lw=3)\n",
    "print(ax_temp.get_xlim())\n",
    "ax_temp.plot(np.flip(imp_experimental),np.linspace(0,n,n),'y',lw=3)\n",
    "print(ax_temp.get_xlim())\n",
    "plt.margins(0,0)\n",
    "plt.show()\n",
    "print(shift_bin.min(), shift_bin.max())\n",
    "\n",
    "fig, ax_temp = plt.subplots()\n",
    "fig.set_size_inches((4,20))\n",
    "ax_temp.imshow(np.flipud(hist_imp_16), aspect='auto', extent=[shift_bin_16.min(),shift_bin_16.max(),0,n], origin='lower')\n",
    "print(ax_temp.get_xlim())\n",
    "ax_temp.plot(np.flip(np.mean(imp_samples_16,1)),np.linspace(0,n,n),'r', lw=3)\n",
    "print(ax_temp.get_xlim())\n",
    "ax_temp.plot(np.flip(imp_experimental),np.linspace(0,n,n),'y',lw=3)\n",
    "print(ax_temp.get_xlim())\n",
    "plt.margins(0,0)\n",
    "plt.show()\n",
    "print(shift_bin.min(), shift_bin.max())\n",
    "\n",
    "fig, ax_temp = plt.subplots()\n",
    "fig.set_size_inches((4,20))\n",
    "ax_temp.imshow(np.flipud(hist_imp_32), aspect='auto', extent=[shift_bin_32.min(),shift_bin_32.max(),0,n], origin='lower')\n",
    "print(ax_temp.get_xlim())\n",
    "ax_temp.plot(np.flip(np.mean(imp_samples_32,1)),np.linspace(0,n,n),'r', lw=3)\n",
    "print(ax_temp.get_xlim())\n",
    "ax_temp.plot(np.flip(imp_experimental),np.linspace(0,n,n),'y',lw=3)\n",
    "print(ax_temp.get_xlim())\n",
    "plt.margins(0,0)\n",
    "plt.show()\n",
    "print(shift_bin.min(), shift_bin.max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "AbFw6nTfFfFi",
    "outputId": "b8f6bb72-3a9c-4bd7-cba4-98e47b40f39a"
   },
   "outputs": [],
   "source": [
    "csfont = {'fontname':'serif'}\n",
    "fig, axs = plt.subplots(1,8)\n",
    "fig.set_dpi(300)\n",
    "axs[0].imshow(modelo_referencia, aspect='auto')\n",
    "axs[1].imshow(all_ensemble_facies_8, aspect='auto')\n",
    "axs[2].imshow(all_ensemble_facies_16, aspect='auto')\n",
    "axs[3].imshow(all_ensemble_facies_32, aspect='auto')\n",
    "axs[4].imshow(hist_facies, aspect='auto')\n",
    "axs[5].imshow(facies_es_8, aspect='auto')\n",
    "axs[6].plot(np.flip(sismica_experimental),np.linspace(0,n-1,n-1), lw=1, label='True')\n",
    "axs[6].plot(np.flip(np.mean(sismica_samples_8,1)),np.linspace(0,n-1,n-1), 'r', lw=1, label='ES')\n",
    "axs[6].legend(fontsize=4,loc='lower right')\n",
    "axs[6].margins(0,0)\n",
    "axs[7].imshow(np.flipud(hist_imp), aspect='auto', extent=[np.exp(9.0690473),np.exp(9.80372756),0,n], origin='lower')\n",
    "axs[7].plot(np.flip(np.exp(imp_experimental)),np.linspace(0,n,n), 'y', lw=1, label='True')\n",
    "axs[7].plot(np.flip(np.exp(np.mean(imp_samples_8,1))),np.linspace(0,n,n),'r',lw=1, label='ES')\n",
    "axs[7].legend(fontsize=4,loc='lower right')\n",
    "#plt.margins(0,0)\n",
    "#fig.tight_layout()\n",
    "\n",
    "ffsize = 4\n",
    "\n",
    "axs[0].set_title('Reference Model', size=ffsize, **csfont)\n",
    "axs[1].set_title('Ensemble 8', size=ffsize, **csfont)\n",
    "axs[2].set_title('Ensemble 16', size=ffsize, **csfont)\n",
    "axs[3].set_title('Ensemble 32', size=ffsize, **csfont)\n",
    "axs[4].set_title('Most Likely Facies', size=ffsize, **csfont)\n",
    "axs[5].set_title('Latent Space mean', size=ffsize, **csfont)\n",
    "axs[6].set_title('Seismic', size=ffsize, **csfont)\n",
    "axs[7].set_title('Impedance', size=ffsize, **csfont)\n",
    "\n",
    "#axs[0].set_title('(a)', size=8, **csfont)\n",
    "#axs[1].set_title('(b)', size=8, **csfont)\n",
    "#axs[2].set_title('(c)', size=8, **csfont)\n",
    "#axs[3].set_title('(d)', size=8, **csfont)\n",
    "#axs[4].set_title('(e)', size=8, **csfont)\n",
    "#axs[5].set_title('(f)', size=8, **csfont)\n",
    "\n",
    "\n",
    "#axs[0].margins(0,0)\n",
    "axs[0].tick_params(labelsize=4)\n",
    "axs[0].get_xaxis().set_visible(False)\n",
    "axs[1].get_xaxis().set_visible(False)\n",
    "axs[1].get_yaxis().set_visible(False)\n",
    "axs[2].get_xaxis().set_visible(False)\n",
    "axs[2].get_yaxis().set_visible(False)\n",
    "axs[3].get_xaxis().set_visible(False)\n",
    "axs[3].get_yaxis().set_visible(False)\n",
    "axs[4].get_xaxis().set_visible(False)\n",
    "axs[4].get_yaxis().set_visible(False)\n",
    "axs[5].get_xaxis().set_visible(False)\n",
    "axs[5].get_yaxis().set_visible(False)\n",
    "#axs[2].axis('off')\n",
    "#axs[3].axis('off')\n",
    "axs[6].tick_params(labelsize=4)\n",
    "#axs[4].set_xlim(-0.12,0.12)\n",
    "axs[7].tick_params(labelsize=4)\n",
    "axs[6].get_yaxis().set_visible(False)\n",
    "axs[7].get_yaxis().set_visible(False)\n",
    "#axs[4].axis('off')\n",
    "#axs[5].axis('off')\n",
    "#plt.savefig('drive/My Drive/Codes/ensemble_smoother/es_result_ml_32_2.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "id": "z7H4evVwcWtd",
    "outputId": "2635c11c-4403-4e6a-93fc-4fb1d92a14f6"
   },
   "outputs": [],
   "source": [
    "np.log(axs[5].get_xlim())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "geC0qaf9JUyk",
    "outputId": "02d9120f-8196-4389-cda0-2b882492f3f7"
   },
   "outputs": [],
   "source": [
    "csfont = {'fontname':'serif'}\n",
    "fig, axs = plt.subplots(1,4)\n",
    "fig.set_dpi(300)\n",
    "fig.set_size_inches(4,6)\n",
    "axs[0].imshow(modelo_referencia, aspect='auto')\n",
    "#axs[1].imshow(all_ensemble_facies, aspect='auto')\n",
    "#axs[2].imshow(hist_facies, aspect='auto')\n",
    "#axs[2].imshow(np.mean(all_ensemble_facies,1), aspect='auto')\n",
    "axs[1].imshow(facies_es, aspect='auto')\n",
    "axs[2].plot(np.flip(sismica_experimental),np.linspace(0,127,127), lw=1, label='True')\n",
    "axs[2].plot(np.flip(np.mean(sismica_samples,1)),np.linspace(0,127,127), 'r', lw=1, label='ES')\n",
    "#axs[4].legend(fontsize=4,loc='lower right')\n",
    "axs[2].margins(0,0)\n",
    "# axs[3].set_xlim(np.exp([9.093,9.808]))\n",
    "\n",
    "axs[3].imshow(np.flipud(hist_imp), aspect='auto', extent=[np.exp(9.135140927930983),np.exp(9.74808053081319),0,128], origin='lower')\n",
    "axs[3].plot(np.flip(np.exp(np.mean(imp_samples,1))),np.linspace(0,128,128),'r',lw=1, label='ES')\n",
    "axs[3].plot(np.flip(np.exp(imp_experimental)),np.linspace(0,128,128), 'y', lw=1, label='True')\n",
    "#axs[3].set_xlim(np.exp(ax_temp.get_xlim()[0]),np.exp(ax_temp.get_xlim()[1]))\n",
    "#axs[5].legend(fontsize=4,loc='lower right')\n",
    "#plt.margins(0,0)\n",
    "#fig.tight_layout()\n",
    "\n",
    "#axs[0].set_title('Reference Model', size=5, **csfont)\n",
    "#axs[1].set_title('Ensemble', size=5, **csfont)\n",
    "#axs[2].set_title('Most Likely Facies', size=5, **csfont)\n",
    "#axs[3].set_title('Latent Space mean', size=5, **csfont)\n",
    "#axs[4].set_title('Seismic', size=5, **csfont)\n",
    "#axs[5].set_title('Impedance', size=5, **csfont)\n",
    "\n",
    "axs[0].set_title('(a)', size=10, **csfont)\n",
    "axs[1].set_title('(b)', size=10, **csfont)\n",
    "axs[2].set_title('(c)', size=10, **csfont)\n",
    "axs[3].set_title('(d)', size=10, **csfont)\n",
    "#axs[4].set_title('(e)', size=8, **csfont)\n",
    "#axs[5].set_title('(f)', size=8, **csfont)\n",
    "\n",
    "\n",
    "axs[0].axis('off')\n",
    "#axs[1].axis('off')\n",
    "#axs[2].axis('off')\n",
    "axs[1].axis('off')\n",
    "axs[2].tick_params(labelsize=6)\n",
    "axs[2].set_xlim(-0.12,0.12)\n",
    "axs[3].tick_params(labelsize=6)\n",
    "\n",
    "#axs[3].get_xlim()\n",
    "axs[2].get_yaxis().set_visible(False)\n",
    "axs[3].get_yaxis().set_visible(False)\n",
    "#axs[4].axis('off')\n",
    "#axs[5].axis('off')\n",
    "#plt.savefig('drive/My Drive/Codes/ensemble_smoother/es_result_ml.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "v4JCIY-7MvF4",
    "outputId": "7b467d8d-b071-4ff2-ff96-b5112af671c6"
   },
   "outputs": [],
   "source": [
    "qtd = 5000\n",
    "input_img = np.random.normal(size=(qtd, 16, 1))\n",
    "output_optim = gan.generator.predict(input_img)\n",
    "print(output_optim.shape)\n",
    "fig = plt.figure(dpi=150)\n",
    "fig.colorbar(plt.imshow(np.round((0.5*output_optim+0.5).reshape(qtd,128)[:100,:41].transpose()*3)))\n",
    "plt.axis('off')\n",
    "plt.show()\n",
    "\n",
    "model_encoder = Sequential()\n",
    "model_encoder.add(Conv1D(32, kernel_size=3, strides=2, input_shape=gan.img_shape, padding=\"same\"))\n",
    "model_encoder.add(LeakyReLU(alpha=0.2))\n",
    "model_encoder.add(Dropout(0.25))\n",
    "model_encoder.add(Conv1D(64, kernel_size=3, strides=2, padding=\"same\"))\n",
    "#model.add(ZeroPadding2D(padding=((0,1),(0,1))))\n",
    "model_encoder.add(BatchNormalization(momentum=0.8))\n",
    "model_encoder.add(LeakyReLU(alpha=0.2))\n",
    "model_encoder.add(Dropout(0.25))\n",
    "model_encoder.add(Conv1D(128, kernel_size=3, strides=2, padding=\"same\"))\n",
    "model_encoder.add(BatchNormalization(momentum=0.8))\n",
    "model_encoder.add(LeakyReLU(alpha=0.2))\n",
    "model_encoder.add(Dropout(0.25))\n",
    "model_encoder.add(Conv1D(256, kernel_size=3, strides=1, padding=\"same\"))\n",
    "model_encoder.add(BatchNormalization(momentum=0.8))\n",
    "model_encoder.add(LeakyReLU(alpha=0.2))\n",
    "model_encoder.add(Dropout(0.25))\n",
    "model_encoder.add(Flatten())\n",
    "model_encoder.add(Dense(16, activation='linear'))\n",
    "\n",
    "X = output_optim\n",
    "Y = input_img.reshape(qtd, gan.latent_dim)\n",
    "\n",
    "model_encoder.compile(loss='mse', optimizer='adam')\n",
    "start_time = time.time()\n",
    "model_encoder.fit(X, Y, epochs=20, batch_size=16, validation_split=0.2, verbose=True)\n",
    "print('Training pos encoder model time (s): ', (time.time() - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "5rtnA3KFBAla",
    "outputId": "326b2023-1d1a-45ee-c6a4-552997f4fdb3"
   },
   "outputs": [],
   "source": [
    "from random import randint\n",
    "i = randint(0,X.shape[0])\n",
    "i = 1331\n",
    "T = X[i:i+1,:,:]\n",
    "T[0,:,:] = modelo_referencia\n",
    "pred_encoder = np.array(model_encoder.predict(T))\n",
    "print(pred_encoder)\n",
    "plt.hist(pred_encoder, bins=4)\n",
    "plt.show()\n",
    "fake_real = gan.generator.predict(np.expand_dims(pred_encoder, axis=2))\n",
    "print(fake_real.shape)\n",
    "\n",
    "out_plot = 0.5*np.array([fake_real[0,:,:], T[0,:,:]])+0.5\n",
    "print(out_plot.shape)\n",
    "\n",
    "fig = plt.figure(dpi=150, figsize=(6,6))\n",
    "fig.colorbar(plt.imshow(np.around(out_plot.reshape(2,128)[:,:].transpose()*3)))\n",
    "plt.axis('off')\n",
    "plt.show()\n",
    "\n",
    "fig = plt.figure(dpi=150, figsize=(6,6))\n",
    "fig.colorbar(plt.imshow(pred_encoder.transpose()))\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [
    "h2W6AUtJmcCd"
   ],
   "name": "esmda_gan_2D.ipynb",
   "provenance": []
  },
  "interpreter": {
   "hash": "1baa965d5efe3ac65b79dfc60c0d706280b1da80fedb7760faf2759126c4f253"
  },
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": ""
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}